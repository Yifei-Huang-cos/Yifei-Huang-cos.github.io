[{"title":"本地部署embedding模型和简单应用","path":"/posts/e871c135/","content":"本地部署文本嵌入模型：从Qwen3BGE到LangChain整合实践Embedding是大语言模型应用的核心基础能力，无论是语义检索、相似度计算还是RAG场景，都离不开高质量的文本向量生成。本文从实战角度，讲解如何本地部署主流的中文嵌入模型，并整合到LangChain生态中，同时结合实际案例演示文本向量化与语义检索的完整流程。 一、为什么选择本地部署嵌入模型？在实际业务中，本地部署嵌入模型相比调用第三方API（如OpenAI Embeddings）有明显优势： 数据安全：文本数据无需对外传输，避免隐私泄露风险 成本可控：无按量计费的API调用成本，适合大规模文本处理 离线可用：无需依赖网络，服务稳定性更高 定制化：可针对中文场景优化，适配特定领域数据 本文讲解两款主流中文嵌入模型的本地部署： 通义千问Qwen3嵌入模型：阿里出品，1536维向量，中文适配性优秀 BGE-large-zh-v1.5：智源研究院出品，1024维向量，轻量化且精度均衡 二、环境准备与基础配置1. 核心依赖安装# 基础嵌入模型依赖pip install sentence-transformers transformers# LangChain生态（适配与应用）pip install langchain langchain-core langchain-huggingface# 数据处理依赖pip install pandas numpy torch# 模型下载（可选，ModelScope）pip install modelscope 2. 模型路径配置建议将模型路径抽离为配置文件，便于统一管理： # env_utils.pyimport os# BGE-large-zh-v1.5 本地路径LOCAL_MODEL_PATH_BGE = /path/to/your/bge-large-zh-v1.5# Qwen3 嵌入模型本地路径LOCAL_MODEL_PATH_QWEN3 = /path/to/your/qwen3-embedding# OpenAI API配置（可选，对比测试用）OPENAI_API_KEY = your-api-keyOPENAI_BASE_URL = your-api-base-url 三、核心模型部署实战1. BGE-large-zh-v1.5 本地部署BGE是中文场景下的经典嵌入模型，轻量化且效果优异： from langchain_huggingface import HuggingFaceEmbeddingsfrom env_utils import LOCAL_MODEL_PATH_BGEdef init_local_bge_embeddings(): 加载本地BGE-large-zh-v1.5模型，适配LangChain接口 model_kwargs = device: cpu, trust_remote_code: True # 关键参数：BGE必须开启归一化，否则相似度计算偏差大 encode_kwargs = normalize_embeddings: True embeddings = HuggingFaceEmbeddings( model_name=LOCAL_MODEL_PATH_BGE, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, cache_folder=None # 禁用HuggingFace缓存，强制使用本地 ) return embeddings# 测试验证if __name__ == __main__: bge_embeddings = init_local_bge_embeddings() test_text = 这是测试本地BGE大模型的中文句子 embedding = bge_embeddings.embed_query(test_text) print(f测试文本：test_text) print(f嵌入向量长度：len(embedding)) # large版输出1024 print(f向量前5个值：embedding[:5]) # 验证归一化 import numpy as np norm = np.linalg.norm(embedding) print(f向量模长：norm:.6f（应接近1.0）) 关键注意点：BGE模型必须开启normalize_embeddings=True，否则生成的向量未归一化，余弦相似度计算结果会严重失真。 2. Qwen3 嵌入模型部署与LangChain整合Qwen3嵌入模型维度更高（1536维），中文语义表达更精准。需自定义类适配LangChain的Embeddings接口： import torchfrom langchain_core.embeddings import Embeddingsfrom sentence_transformers import SentenceTransformerfrom env_utils import LOCAL_MODEL_PATH_QWEN3class CustomQwen3Embeddings(Embeddings): 自定义Qwen3嵌入模型类，适配LangChain标准接口 def __init__(self, device=cpu): self.model = SentenceTransformer( LOCAL_MODEL_PATH_QWEN3, model_kwargs= torch_dtype: torch.float32, trust_remote_code: True , tokenizer_kwargs= padding_side: left # Qwen官方推荐配置 ) def embed_query(self, text: str) - list[float]: 单查询文本向量化 return self.embed_documents([text])[0] def embed_documents(self, texts: list[str]) - list[list[float]]: 多文档批量向量化 return self.model.encode(texts).tolist()# 测试验证if __name__ == __main__: qwen3 = CustomQwen3Embeddings() resp = qwen3.embed_documents([i love you, 你好]) print(f单个嵌入向量维度：len(resp[0])) # Qwen3输出1536 适配说明：继承LangChain的Embeddings抽象类，实现embed_query和embed_documents两个核心方法，即可让Qwen3模型无缝接入LangChain生态。 3. OpenAI Embeddings 对比（可选）如需对比第三方API效果，可参考： from langchain_openai import OpenAIEmbeddingsfrom env_utils import OPENAI_API_KEY, OPENAI_BASE_URLopenai_embedding = OpenAIEmbeddings( api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL, model=text-embedding-3-large, dimensions=256,)resp = openai_embedding.embed_documents([I like studying LLM!, 今天天气很好])print(f向量长度：len(resp[0])) # 输出256 四、实战案例：美食评论语义检索基于本地部署的BGE模型，实现美食评论的语义检索，完整流程包括：文本向量化存储 → 输入查询向量生成 → 余弦相似度计算 → TopN相似结果返回。 1. 评论数据向量化并保存import pandas as pdimport numpy as npfrom env_utils import LOCAL_MODEL_PATH_BGEfrom langchain_huggingface import HuggingFaceEmbeddings# 初始化BGE模型model_kwargs = device: cpu, trust_remote_code: Trueencode_kwargs = normalize_embeddings: Truebge_embeddings = HuggingFaceEmbeddings( model_name=LOCAL_MODEL_PATH_BGE, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, cache_folder=None)def text_to_embedding(text): 单条文本转向量 return bge_embeddings.embed_documents([text])[0]def embedding_to_file(source_file, output_file): 批量处理评论数据并保存 df = pd.read_csv(source_file, index_col=0) df = df[[Time,ProductId,UserId,Summary,Text]] df[text_content] = Summary: + df.Summary.fillna().str.strip() + ; Text: + df.Text.fillna().str.strip() # 批量生成嵌入向量 print(f开始编码 len(df) 条文本...) df[embedding] = df.text_content.apply(lambda x: text_to_embedding(x)) df.to_csv(output_file) print(f已保存至 output_file)if __name__ == __main__: embedding_to_file(data/fine_food_reviews_1k.csv, data/output_embedding.csv) 2. 语义检索实现import astimport pandas as pdimport numpy as npdef cosine_similarity(a, b): 余弦相似度计算（向量需归一化） return np.dot(a, b)def search_text(input_text, embedding_file, top_n=3): 根据输入查询，返回最相似的top_n条评论 df_data = pd.read_csv(embedding_file) # 字符串转数组 df_data[embedding_vector] = df_data[embedding].apply(ast.literal_eval) # 生成查询向量 input_vector = text_to_embedding(input_text) # 计算相似度 df_data[similarity] = df_data.embedding_vector.apply( lambda x: cosine_similarity(input_vector, x) ) # 返回TopN结果 resp = ( df_data.sort_values(similarity, ascending=False) .head(top_n) .text_content .str.replace(Summary:, ) .str.replace(; Text:, ) ) # 输出结果 print(f查询：input_text ) for idx, r in enumerate(resp, 1): print(f【相似结果idx 相似度：df_data.sort_values(similarity, ascending=False).iloc[idx-1].similarity:.4f】) print(r[:200] + ... if len(r) 200 else r) print(- * 50)if __name__ == __main__: search_text(delicious beans, data/output_embedding.csv) 3. 常见问题与排查 现象 可能原因 解决方案 OOM内存溢出 模型默认加载到GPU 显式设置 device=cpu 向量全为0或模长≠1 BGE未开启归一化 添加 normalize_embeddings=True 模型加载极慢 每次运行重新下载 使用本地路径，设置 cache_folder=None Qwen3输出NaN CPU精度问题 设置 torch_dtype=torch.float32 trust_remote_code报错 安全策略拦截 显式设置 trust_remote_code=True 批量编码效率低 单条循环编码 直接传入list，利用batch处理 五、核心总结与实践建议1. 模型选型建议 场景 推荐模型 维度 优势 轻量化、高吞吐 BGE-large-zh-v1.5 1024 速度快，资源占用低 高精度、长文本 Qwen3嵌入模型 1536 语义理解能力强 对比验证 OpenAI Embeddings 可变 基准测试参考 2. 关键配置要点 BGE模型：必须开启normalize_embeddings=True Qwen3模型：设置padding_side=left、trust_remote_code=True CPU推理：精度设为torch.float32，避免NaN 批量处理：使用embed_documents传入list，提升吞吐量 3. 后续拓展方向 向量检索加速：10万条以上建议使用FAISS、Chroma等向量数据库替代全表扫描 RAG应用集成：通过LangChain的VectorStore接口对接本地知识库 模型量化：INT8量化可降低显存占用，提升推理速度 指令前缀优化：检索时在查询前添加”为这个句子生成表示：”，可提升BGE效果 六、结语本文详细讲解了本地部署Qwen3、BGE两款中文嵌入模型的方法，以及如何适配LangChain生态，并通过美食评论语义检索案例，展示了文本嵌入的完整应用流程。本地部署嵌入模型既能保障数据安全，又能有效控制业务成本，是中文大模型应用落地的重要基础能力。 以上代码均已通过实际测试，可直接复制使用。如遇到环境兼容性问题，建议检查transformers和sentence-transformers版本，或参考各模型官方仓库的issue解决方案。","categories":["编程"]},{"title":"从0搭建多模态聊天机器人","path":"/posts/50b3b349/","content":"从零搭建多模态聊天机器人目标：构建一个支持语音、图片和文字输入的聊天机器人，具备简易Web前端和上下文记忆功能。 一、环境准备1. 模型选择与部署我们选择Qwen-2.5-Omni-3B模型，这是一个支持多模态输入的开源模型。可以选择： 私有化部署：只需在一张4090显卡（24G显存以上）上部署 API调用（附录提供方法） 注意：如果您已熟悉模型部署，可跳过下面一段 2. 服务器连接与配置方法一：使用PyCharm SSH连接 打开PyCharm：设置 → Python解释器 → 添加解释器 → 基于SSH 输入服务器IP、用户名和密钥文件 方法二：直接使用服务器终端3. 安装依赖包在服务器终端执行以下命令： # 核心依赖pip install langchain langchain_openai langchain_community# 模型部署与推理pip install vllm vllm[audio]# 模型下载pip install modelscope# 前端与工具pip install gradio pillow pydub sqlalchemy 4. 下载模型创建 download_model.py 文件： from modelscope import snapshot_downloadmodel_dir = snapshot_download(Qwen/Qwen2.5-Omni-3B)print(f模型下载完成，保存至: model_dir) 运行后等待约10分钟完成下载。 5. 启动模型服务python -m vllm.entrypoints.openai.api_server \\ --model /path/to/Qwen2.5-Omni-3B \\ # 替换为您的实际路径 --served-model-name qwen2.5-omni-3b \\ --max-model-len 8192 \\ --host 0.0.0.0 \\ --port 6006 \\ --dtype auto \\ --gpu-memory-utilization 0.9 \\ --enable-auto-tool-choice 验证：访问 http://服务器IP:6006/docs 查看API文档。 二、项目构建1. 项目结构multi-modal-chatbot/├── .env # 环境变量├── my_llm.py # 模型配置├── utils.py # 工具函数├── main.py # 主程序├── requirements.txt # 依赖列表└── chat_history.db # 数据库（自动生成） 2. 环境配置为代码的健全性，我们不在main函数中调用大模型，而是放在my_llm文件里方便修改 .env 文件：LOCAL_BASE_URL=http://your-server-ip:6006/v1 my_llm.py - 模型配置：import osfrom dotenv import load_dotenvfrom langchain_openai import ChatOpenAI# 加载环境变量load_dotenv()# 配置多模态模型multiModal_llm = ChatOpenAI( model=qwen2.5-omni-3b, api_key=os.getenv(OPENAI_API_KEY, xx), # 本地部署可为任意值 base_url=os.getenv(LOCAL_BASE_URL), temperature=0.7, max_tokens=2048,) 3. 工具函数 (utils.py)这里主要负责将音频图片文件转化为base64格式 import base64import ioimport osimport tempfilefrom PIL import Imagefrom pydub import AudioSegmentdef image_to_base64(image_path: str) - dict: 将图片转换为Base64格式 try: with Image.open(image_path) as img: # 统一格式处理 if img.mode in (RGBA, LA): background = Image.new(RGB, img.size, (255, 255, 255)) background.paste(img, mask=img.split()[-1]) img = background buf = io.BytesIO() img.save(buf, format=JPEG, quality=85) b64 = base64.b64encode(buf.getvalue()).decode(utf-8) return type: image_url, image_url: url: fdata:image/jpeg;base64,b64, detail: low # 可选: low, high, auto except Exception as e: print(f[ERROR] 图片处理失败: e) return Nonedef audio_to_base64(audio_path: str) - dict: 将音频转换为Base64格式 try: # 统一转换为WAV格式 audio = AudioSegment.from_file(audio_path) audio = audio.set_frame_rate(16000).set_channels(1) with tempfile.NamedTemporaryFile(suffix=.wav, delete=False) as tmp: audio.export(tmp.name, format=wav) tmp_path = tmp.name # 读取并编码 with open(tmp_path, rb) as f: b64 = base64.b64encode(f.read()).decode(utf-8) # 清理临时文件 os.unlink(tmp_path) return type: audio_url, audio_url: url: fdata:audio/wav;base64,b64, duration: len(audio) / 1000 # 秒 except Exception as e: print(f[ERROR] 音频处理失败: e) return None 4. 主程序 (main.py)import uuidimport gradio as grfrom sqlalchemy import create_enginefrom langchain_core.messages import HumanMessagefrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables import RunnableWithMessageHistoryfrom langchain_community.chat_message_histories import SQLChatMessageHistoryfrom my_llm import multiModal_llmfrom utils import image_to_base64, audio_to_base64# ========== 1. 对话链配置 ==========prompt = ChatPromptTemplate.from_messages([ (system, 你是一个多模态AI助手，可以处理文本、图片和语音输入。请用中文友好地回答用户的问题。), MessagesPlaceholder(variable_name=messages), (system, 当前对话历史:), MessagesPlaceholder(variable_name=history),])chain = prompt | multiModal_llm# ========== 2. 记忆管理 ==========def get_session_history(session_id: str): 获取或创建会话历史 engine = create_engine(sqlite:///chat_history.db) return SQLChatMessageHistory( session_id=session_id, connection=engine, table_name=chat_history )# 包装对话链chain_with_history = RunnableWithMessageHistory( chain, get_session_history, input_messages_key=messages, history_messages_key=history)# 生成唯一会话IDsession_id = str(uuid.uuid4())# ========== 3. 核心处理函数 ==========def process_inputs(text: str, audio: str, image: str, chat_history: list): 处理用户输入并生成响应 # 构建多模态内容 content = [] user_display = [] # 处理图片 if image: img_data = image_to_base64(image) if img_data: content.append(img_data) user_display.append([图片]) # 处理音频 if audio: audio_data = audio_to_base64(audio) if audio_data: content.append(audio_data) user_display.append([语音]) # 处理文本 if text and text.strip(): content.append(type: text, text: text) user_display.append(text) # 无有效输入 if not content: chat_history.append((, 请输入文本、上传图片或录制语音！)) return , None, None, chat_history # 构建用户消息 user_message = HumanMessage(content=content) display_text = .join(user_display) if user_display else [多媒体内容] try: # 调用模型 response = chain_with_history.invoke( messages: [user_message], config=configurable: session_id: session_id ) # 更新聊天历史 chat_history.append((display_text, response.content)) except Exception as e: error_msg = f请求失败: str(e) print(f[ERROR] error_msg) chat_history.append((display_text, error_msg)) # 清空输入 return , None, None, chat_history# ========== 4. 前端界面 ==========def create_interface(): 创建Gradio界面 with gr.Blocks( title=多模态聊天机器人, theme=gr.themes.Soft(), css= .chatbot min-height: 500px; .input-row margin-top: 20px; ) as demo: gr.Markdown( # 🤖 多模态聊天机器人 支持**文字**、**图片**、**语音**输入 | 具备上下文记忆 ) # 聊天窗口 chatbot = gr.Chatbot( label=对话历史, height=500, bubble_full_width=False, show_copy_button=True ) # 输入区域 with gr.Row(variant=panel, elem_classes=input-row): text_input = gr.Textbox( placeholder=输入文字消息..., label=文本输入, scale=4, container=False ) submit_btn = gr.Button(发送, variant=primary, scale=1) # 多媒体输入 with gr.Row(): audio_input = gr.Audio( sources=[microphone], type=filepath, label=语音输入, interactive=True ) image_input = gr.Image( type=filepath, label=图片上传, sources=[upload], interactive=True ) # 绑定事件 submit_btn.click( fn=process_inputs, inputs=[text_input, audio_input, image_input, chatbot], outputs=[text_input, audio_input, image_input, chatbot], queue=True ) # 回车键提交 text_input.submit( fn=process_inputs, inputs=[text_input, audio_input, image_input, chatbot], outputs=[text_input, audio_input, image_input, chatbot], queue=True ) # 清理按钮 with gr.Row(): clear_btn = gr.Button(清空对话, variant=secondary) clear_btn.click( fn=lambda: ([], , None, None), outputs=[chatbot, text_input, audio_input, image_input], queue=False ) return demo# ========== 5. 启动应用 ==========if __name__ == __main__: demo = create_interface() demo.queue( max_size=20, default_concurrency_limit=5 ) demo.launch( server_name=0.0.0.0, server_port=7860, share=False, # 设为True可生成公网链接 show_error=True, debug=True ) 三、运行与测试1. 启动应用python main.py 2. 访问地址 本地: http://localhost:7860 局域网: http://[服务器IP]:7860 3. 功能测试 ✅ 文本对话 ✅ 图片识别与描述 ✅ 语音转文字 ✅ 上下文记忆 ✅ 对话历史保存 四、常见问题Q1: 显存不足 降低 max_model_len 参数 设置 --gpu-memory-utilization 0.8 使用量化版本模型 Q2: 音频处理失败 安装 ffmpeg: sudo apt install ffmpeg 检查音频格式支持 Q3: 响应缓慢 调整 max_tokens 限制 使用 stream=True 启用流式输出 升级服务器配置 附录：API调用方案如果不方便部署，可以使用阿里云、OpenAI等API服务： # 阿里云通义千问from langchain_openai import ChatOpenAIllm = ChatOpenAI( model=qwen-max, base_url=https://dashscope.aliyuncs.com/compatible-mode/v1, api_key=your-api-key)# OpenAI GPT-4Vllm = ChatOpenAI( model=gpt-4-vision-preview, api_key=your-openai-key) 总结本文详细介绍了如何从零构建一个功能完整的多模态聊天机器人。通过私有化部署Qwen-2.5-Omni模型，结合LangChain和Gradio，我们实现了： 多模态支持：文本、图片、语音输入 上下文记忆：SQLite持久化存储 Web界面：直观的用户交互 可扩展架构：模块化设计，易于扩展 项目代码已开源，可根据实际需求进行调整和优化。欢迎您宝贵的建议~","categories":["编程"]},{"title":"基于轻量化CNN实现自动驾驶方向预测","path":"/posts/105e3bfd/","content":"基于轻量化CNN实现自动驾驶方向预测在深度学习计算机视觉领域，CNN（卷积神经网络）凭借强大的空间特征提取能力，成为自动驾驶环境感知任务的核心技术支撑。真实自动驾驶场景复杂且数据获取成本高，对于新手而言，很难快速上手并验证模型效果。 本次我们将通过极致简化但贴合核心逻辑的场景，从零搭建一个轻量化CNN模型，实现「车辆姿态自适应（纵向横向切换）+ 车头与行驶方向强绑定」的自动驾驶方向预测，全程代码可复现、无环境坑，最终达成测试集99.2%的高准确率，帮你快速打通「数据生成→模型搭建→训练评估」的深度学习实战闭环。 一、项目核心背景与目标1. 项目背景自动驾驶的核心流程可拆解为「环境感知→决策规划→执行控制」，其中环境感知是基础——车辆需要通过摄像头、激光雷达等设备获取自身与环境的相对位置，再通过模型输出最优行驶策略。真实场景中，车辆姿态、道路环境、障碍物、光照变化等因素都会增加模型学习难度，甚至导致数据标注成本激增（需人工标注大量车辆位置、行驶方向样本）。 为降低实战门槛，同时保留核心逻辑，本次项目做了针对性简化设计，聚焦「车辆位置→行驶方向」的映射关系，剥离无关干扰： 环境简化：用「32×32黑白方格图」模拟平坦无障碍物的行驶道路，方格线清晰可辨，便于模型捕捉车辆与环境的相对位置； 车辆简化：用「自适应姿态长方体」模拟车辆，避免复杂车辆轮廓建模，同时支持姿态切换（上下行驶纵放、左右行驶横放），贴合真实车辆行驶逻辑； 方向标记：用「内置深灰色小方块」标记车头，与黑色车身形成明显视觉区分，且车头方向直接对应车辆行驶方向，避免方向歧义； 标注简化：通过代码自动生成标注数据，无需人工标注，降低实战成本，同时保证数据标注的准确性和一致性。 通过以上简化，我们可专注于理解CNN的空间特征提取逻辑、数据集生成原理、模型训练与优化流程，为后续学习复杂自动驾驶场景打下坚实基础。 2. 核心目标 实现车辆姿态自适应生成：根据车辆行驶方向（上下左右）自动切换车身姿态，上下行驶时纵放（尺寸8×4）、左右行驶时横放（尺寸4×8），保证视觉合理性与真实行驶逻辑一致； 完成车头与行驶方向强绑定：车头朝向直接决定车辆可行驶范围，模型仅输出「车头方向」或「直行」两种结果，无跨方向、不合理的预测输出，贴合真实驾驶决策逻辑； 构建自动化数据集生成流程：通过代码随机生成指定数量（5000张）的样本图，自动完成环境、车辆、车头的绘制及方向标签标注，保证数据集的结构化与可复现性； 搭建轻量化CNN模型并掌握其设计逻辑：理解卷积层、池化层、全连接层的作用，掌握参数设置（滤波器数量、 kernel尺寸、 dropout比例等）的设计考量，实现模型轻量化（低参数、快训练）； 完成模型训练、优化与全面评估：掌握模型编译、训练回调函数的使用，通过测试集验证模型效果，分析训练曲线，达成高准确率（≥99%），形成完整的深度学习实战闭环。 二、前期准备1. 环境配置本次项目采用「Python 3.11 + TensorFlow 2.20.0」组合，该组合兼容性强、稳定性高，可有效避免新版Python与TensorFlow的适配问题，核心依赖库及作用如下（明确每一步依赖的必要性，便于新手理解）： tensorflow==2.20.0：核心框架，用于搭建、编译、训练轻量化CNN模型，提供丰富的神经网络层（卷积层、池化层等）与回调函数，简化模型开发流程； numpy：用于数据处理与矩阵运算，核心作用是将生成的图像转换为数组格式、处理数据维度（如增加通道维度）、生成随机坐标等，是深度学习数据处理的基础； opencv-python：用于图像绘制，核心作用是绘制方格背景、自适应姿态车辆、深灰色车头，实现样本图的可视化生成，无需手动制作图像； matplotlib：用于结果可视化，核心作用是绘制训练损失准确率曲线、展示样本验证图，便于分析模型训练效果与数据生成质量； scikit-learn：用于数据集划分，核心作用是将生成的样本按7:2:1的比例划分为训练集、验证集、测试集，确保模型训练、优化、评估的独立性，避免数据泄露。 环境搭建步骤 安装Python 3.11：避免安装Python 3.12及以上版本，此类新版本与TensorFlow 2.20.0存在适配问题，可能导致依赖库安装失败或模型训练报错； 新建虚拟环境并激活：虚拟环境可隔离不同项目的依赖库，避免因依赖版本冲突导致项目无法运行，建议使用conda或venv创建（新手推荐venv，操作简单）；。 2. 核心思路梳理（详细拆解，闭环逻辑）：Text数据自动生成（环境+车辆+车头+标签）→ 数据预处理（格式转换+one-hot编码）→ 数据集划分 → 轻量化CNN模型搭建 → 模型编译与训练（添加回调优化）→ 模型评估（测试集验证）→ 结果可视化（样本+训练曲线） 逐步骤详细解读核心思路： 数据自动生成：这是本次项目的基础的核心步骤，也是降低实战门槛的关键。通过编写专门的函数，自动完成“方格背景绘制→车辆姿态切换→车头绘制→方向标签标注”，无需人工干预，既保证数据质量，又提升效率。核心设计逻辑是“姿态与方向绑定”——上下行驶对应纵向车辆，左右行驶对应横向车辆，车头方向直接决定标签判断维度（纵向车辆判断纵向偏移，横向车辆判断横向偏移）； 数据预处理：生成的原始数据（图像、标签）无法直接输入CNN模型，需进行两项关键处理：一是格式转换（将图像列表转换为numpy数组，并增加通道维度，适配CNN单通道灰度图输入格式）；二是标签one-hot编码（将整数标签转换为one-hot向量，适配多分类交叉熵损失函数，避免标签权重不均导致的训练偏差）； 数据集划分：按7:2:1的比例划分为训练集、验证集、测试集，三者作用明确且独立：训练集用于模型学习数据规律（车辆位置与方向的映射）；验证集用于训练过程中优化模型（调整权重、避免过拟合）；测试集用于最终评估模型泛化能力（未见过的数据上的表现），确保评估结果真实可靠； 轻量化CNN模型搭建：核心设计原则是“轻量化、高效果”，避免复杂结构导致的训练缓慢（普通CPU可快速运行）。搭建3层卷积层（逐步提取空间特征）+ 2层全连接层（分类决策），配合批量归一化（加速收敛）、最大池化（降低维度、减少计算量）、dropout（防止过拟合），平衡特征提取能力与训练效率； 模型编译与训练：编译时选择合适的优化器、损失函数、评估指标——Adam优化器（收敛稳定、学习率自适应，适合新手使用）、多分类交叉熵损失（适配one-hot标签多分类任务）、准确率（直观评估模型分类效果）。训练时添加回调函数（早停法、模型保存），优化训练过程：早停法避免无效训练（验证集损失10轮不下降则停止），模型保存自动留存最优模型，无需手动记录； 模型评估：用测试集对训练好的模型进行评估，输出测试集损失与准确率，核心判断标准是“损失低、准确率高”，且测试集准确率与训练集、验证集准确率差距较小，说明模型无过拟合、泛化能力强； 结果可视化：通过两种可视化方式验证项目效果：一是样本验证图（展示5张随机样本，直观查看车辆姿态、车头方向、标签的绑定效果，确认数据生成无误）；二是训练曲线（损失曲线、准确率曲线，分析训练趋势，判断模型是否收敛、是否过拟合）。 三、实现思路1. 核心参数配置（支持车辆姿态切换，可复现性设计）参数配置是项目可复现、可配置的关键，所有参数均有明确设计考量，并非随意设置，核心参数及详细解读如下（重点说明姿态切换相关参数）： 图像尺寸参数（IMAGE_SIZE32）：设置为32×32，核心考量是“轻量化”——减小图像尺寸可降低CNN模型的计算量，让普通CPU能快速训练，同时32×32的尺寸足够容纳车辆（8×4或4×8）与方格背景，避免图像过于拥挤； 车辆姿态参数（CAR_LONG8、CAR_SHORT4）：这是车辆姿态切换的核心参数，设计逻辑是“姿态与方向匹配”——上下行驶时，车辆纵向摆放，此时车身纵向长度（CAR_LONG8）大于横向宽度（CAR_SHORT4），贴合真实车辆上下行驶的视觉效果；左右行驶时，互换两个参数，车辆横向摆放（横向长度8、纵向宽度4），确保姿态合理； 车头参数（HEAD_SIZE2）：设置为2×2的小方块，核心考量是“与车身区分、不遮挡车身”——2×2的尺寸足够清晰标记车头方向，同时不会过大遮挡车身（8×4或4×8），颜色选择深灰色（与黑色车身区分），便于模型捕捉车头特征（车头方向是标签判断的关键）； 偏移阈值（OFFSET8）：用于判断车辆是否偏离背景中心，进而确定行驶方向标签。设计逻辑是“背景中心±8像素”作为直行与转向的分界，32×32图像的中心是16像素，偏移超过8像素（即车身中心超出8-24像素范围），则判断为对应方向行驶，否则为直行，该阈值经过多次测试，可确保标签判断合理，无过多歧义样本； 可复现性参数（SEED42）：固定随机种子，核心作用是“确保每次运行代码，生成的数据集、模型训练结果完全一致”——避免随机因素（如随机生成车辆坐标、随机划分数据集）导致的结果波动，便于新手复现项目、调试代码； 数据集与训练参数（NUM_SAMPLES5000、EPOCHS20、BATCH_SIZE64）：5000张样本足够模型学习规律（避免样本过少导致过拟合），且不会过多占用内存；20轮训练可确保模型收敛（配合早停法，无需训练满20轮）；64的批次大小适配普通CPU，平衡训练速度与内存占用。 此外，定义行驶方向标签（DIRECTIONS）与车头方向（MOVE_DIRECTIONS），建立两者的映射关系，确保标签与车头方向、车辆姿态强绑定，避免方向歧义（仅输出up、down、left、right、straight五种标签，对应5分类任务）。 2. 核心函数设计思路（车辆生成+姿态切换+方向绑定）核心函数（create_car_image）是数据自动生成的核心，负责单张样本图的完整生成，函数内部逻辑拆解为4个关键模块，每个模块的设计思路、实现逻辑详细说明如下： 模块1：方格背景生成核心目的是模拟平坦行驶道路，便于模型捕捉车辆与环境的相对位置。设计思路：首先初始化32×32的白色背景（灰度图，255表示白色），然后绘制8×8的方格线（黑色，线宽1）——方格步长为4（32÷84），即每4个像素绘制一条横向、纵向线，形成均匀的方格网格，模拟真实道路的分隔效果，同时方格线可帮助模型定位车辆位置（判断车辆中心与背景中心的偏移）。 模块2：车辆姿态自适应切换这是函数的核心亮点，设计思路是“车头方向决定车辆姿态”——随机选择车头方向（up、down、left、right），然后根据车头方向切换车身尺寸与摆放姿态： 车头方向为up、down（上下行驶）：车辆纵向摆放，车身高度CAR_LONG（8），车身宽度CAR_SHORT（4），确保纵向长度大于横向宽度，贴合上下行驶的视觉逻辑； 车头方向为left、right（左右行驶）：车辆横向摆放，车身高度CAR_SHORT（4），车身宽度CAR_LONG（8），互换尺寸，确保横向长度大于纵向宽度，贴合左右行驶的视觉逻辑。 同时，随机生成车身左上角坐标，核心约束是“车身完全在背景内，不截断”——坐标范围为（0IMAGE_SIZE - 车身宽度，0IMAGE_SIZE - 车身高度），避免车身超出32×32的背景范围，确保样本图的完整性。 模块3：车头绘制（与方向强绑定）核心目的是标记车辆行驶方向，为标签标注提供依据，设计思路是“车头位置与车辆姿态、车头方向完全匹配”，确保车头方向直观、无歧义： 车头朝上（up）：纵向车辆，车头位于车身顶端中心，坐标计算为（车身x坐标 + 车身宽度2 - 车头尺寸2，车身y坐标），确保车头在顶端中心，与车身对齐； 车头朝下（down）：纵向车辆，车头位于车身底端中心，坐标计算为（车身x坐标 + 车身宽度2 - 车头尺寸2，车身y坐标 + 车身高度 - 车头尺寸），避免车头超出车身； 车头朝左（left）：横向车辆，车头位于车身左端中心，坐标计算为（车身x坐标，车身y坐标 + 车身高度2 - 车头尺寸2），确保车头在左端中心； 车头朝右（right）：横向车辆，车头位于车身右端中心，坐标计算为（车身x坐标 + 车身宽度 - 车头尺寸，车身y坐标 + 车身高度2 - 车头尺寸2），避免车头超出车身。 车头颜色设置为深灰色（128），与黑色车身（0）形成明显区分，便于模型快速捕捉车头特征，进而学习车头方向与行驶方向的映射关系。 模块4：方向标签自动标注核心逻辑是“根据车辆中心与背景中心的偏移量，结合车头方向，自动标注标签”，确保标签标注准确、贴合真实驾驶逻辑，具体设计思路如下： 计算车辆中心与背景中心：车辆中心根据车身姿态自适应计算（横向车辆中心车身x+宽度2、y+高度2；纵向车辆同理），背景中心固定为16（322）； 标签初始化：默认标签为“直行（straight）”，对应标签4，设计逻辑是“大多数车辆处于中心附近，默认直行，减少极端标签占比”； 标签判断逻辑（核心：车头方向决定偏移判断维度）： 车头朝上（up）：仅判断纵向偏移，若车辆中心y坐标 背景中心+偏移阈值（16+824），说明车辆偏下，需朝上行驶，标注标签0（up）；否则保持直行； 车头朝下（down）：仅判断纵向偏移，若车辆中心y坐标 背景中心-偏移阈值（16-88），说明车辆偏上，需朝下行驶，标注标签1（down）；否则保持直行； 车头朝左（left）：仅判断横向偏移，若车辆中心x坐标 背景中心+偏移阈值（24），说明车辆偏右，需朝左行驶，标注标签2（left）；否则保持直行； 车头朝右（right）：仅判断横向偏移，若车辆中心x坐标 背景中心-偏移阈值（8），说明车辆偏左，需朝右行驶，标注标签3（right）；否则保持直行。 该设计逻辑的核心优势是“无跨方向标注”——纵向车辆仅判断纵向偏移，横向车辆仅判断横向偏移，避免出现“车头朝上却标注朝左”的不合理情况，贴合真实驾驶中“车头方向决定行驶范围”的逻辑。 3. 数据集生成与预处理思路数据集生成与预处理是模型训练的基础，核心思路是“批量生成、规范格式、合理划分”，确保数据符合CNN模型输入要求，具体拆解如下： （1）批量数据集生成调用核心函数（create_car_image），循环5000次，批量生成样本图，同时保存图像数据、方向标签、车头方向、车辆姿态，便于后续可视化验证。设计思路是“固定随机种子（SEED42）”，确保每次生成的数据集完全一致，便于复现与调试；循环次数对应样本数量（5000张），足够模型学习数据规律，同时避免样本过多导致内存占用过高。 （2）数据格式转换生成的原始数据（图像列表、标签列表）无法直接输入CNN模型，需进行两项关键转换，设计思路贴合CNN模型输入要求： 图像格式转换：将图像列表转换为numpy数组，同时增加通道维度（[…, np.newaxis]），最终格式为（5000, 32, 32, 1）——5000是样本数量，32×32是图像尺寸，1是通道数（灰度图），适配CNN模型“(样本数, 图像高度, 图像宽度, 通道数)”的输入格式； 标签格式转换：将整数标签（0-4）转换为one-hot编码，例如标签0转换为[1,0,0,0,0]，标签4转换为[0,0,0,0,1]。设计思路是“适配多分类交叉熵损失函数”，one-hot编码可避免标签权重不均（如整数标签4的权重高于0），确保模型训练时对每个类别平等对待。 （3）数据集划分采用sklearn的train_test_split函数，按7:2:1的比例划分为训练集、验证集、测试集，设计思路是“兼顾训练效果与评估可靠性”： 第一步：将所有数据按7:3划分为训练集（70%，3500张）与临时集（30%，1500张）； 第二步：将临时集按2:1划分为验证集（20%，1000张）与测试集（10%，500张）； 划分约束：固定随机种子（SEED42），确保划分结果可复现；划分时同时划分图像数据与one-hot标签，确保图像与标签一一对应，避免数据错乱。 4. 轻量化CNN模型搭建思路模型搭建的核心原则是“轻量化、高效果、易训练”，避免复杂结构，普通CPU可快速运行，同时保证特征提取能力，模型结构为“3层卷积模块 + 2层全连接模块”，每层设计思路、参数考量详细拆解如下： （1）卷积模块（3层，特征提取核心）卷积模块的核心作用是提取图像的空间特征（车辆位置、车头方向、车身姿态），每层卷积后配合批量归一化与最大池化，设计思路是“逐步深化特征提取、降低计算量、加速收敛”： 卷积层1：滤波器数量32，kernel尺寸（3,3），激活函数relu。设计思路：32个滤波器用于提取基础空间特征（如方格线、车身轮廓），（3,3）的kernel是卷积层最常用尺寸，可有效捕捉局部特征；relu激活函数可引入非线性，解决线性模型无法拟合复杂数据的问题，同时计算简单、不易梯度消失； 批量归一化（BatchNormalization）：每个卷积层后添加，核心作用是“加速模型收敛、缓解梯度消失”——将卷积层输出的特征图进行归一化处理，使数据分布更均匀，避免因数据尺度差异导致的训练缓慢； 最大池化（MaxPooling2D）：尺寸（2,2），核心作用是“降低特征图维度、减少计算量、保留关键特征”——将2×2的区域取最大值作为输出，可去除冗余信息，同时避免过拟合； 卷积层2：滤波器数量64，kernel尺寸（3,3），激活函数relu。设计思路：滤波器数量翻倍，逐步提取更精细的特征（如车头位置、车身姿态），随着层数增加，特征提取能力逐步增强； 卷积层3：滤波器数量128，kernel尺寸（3,3），激活函数relu。设计思路：继续增加滤波器数量，深化特征提取，捕捉车辆位置与方向的映射关系，为后续分类决策提供足够的特征支撑。 经过3层卷积模块处理后，原始32×32的图像逐步降维为2×2的特征图，滤波器数量从32增加到128，实现“从基础特征到精细特征”的逐步提取，同时通过池化层控制计算量，确保轻量化。 （2）全连接模块（2层，分类决策核心）全连接模块的核心作用是将卷积模块提取的特征图转换为分类结果，设计思路是“逐步压缩特征维度、避免过拟合、输出分类概率”： Flatten（展平层）：核心作用是“连接卷积模块与全连接模块”——将卷积模块输出的2×2×128的特征图（三维）展平为一维向量（512维），适配全连接层的输入格式； 全连接层1（Dense(256, relu)）：输出维度256，激活函数relu。设计思路：将512维特征压缩为256维，进一步筛选关键特征，relu激活函数继续引入非线性，增强模型拟合能力； Dropout(0.4)：核心作用是“防止过拟合”——随机丢弃40%的神经元，避免模型过度依赖某部分特征，增强模型泛化能力（在未见过的数据上表现更好）； 全连接层2（Dense(64, relu)）：输出维度64，激活函数relu。设计思路：继续压缩特征维度，将256维特征压缩为64维，简化分类决策过程，同时保留关键特征； Dropout(0.3)：继续随机丢弃30%的神经元，进一步防止过拟合，平衡模型拟合能力与泛化能力； 输出层（Dense(5, softmax)）：输出维度5（对应5个行驶方向），激活函数softmax。设计思路：softmax激活函数可将输出转换为5个类别的概率分布（概率和为1），对应最大概率的类别即为模型预测的行驶方向，适配多分类任务。 （3）模型轻量化设计亮点该模型总计约24.1万参数（944.02 KB），属于轻量化模型，普通CPU可在几分钟内完成20轮训练，核心轻量化设计亮点： 图像尺寸小（32×32），降低输入数据量； 卷积层kernel尺寸固定为（3,3），计算量小； 全连接层维度逐步压缩（512→256→64→5），减少参数数量； 通过最大池化降低特征图维度，减少后续计算量。 5. 模型编译与训练优化思路模型编译与训练是实现高准确率的关键，核心思路是“合理配置编译参数、优化训练过程、避免无效训练”，具体拆解如下： （1）模型编译编译时需配置优化器、损失函数、评估指标，三者选择均贴合项目需求，设计思路如下： 优化器：Adam，学习率1e-4。设计思路：Adam优化器结合了SGD和RMSprop的优势，收敛稳定、学习率自适应，适合新手使用；学习率设置为1e-4（较小），避免学习率过大导致训练震荡、不收敛，同时保证训练速度； 损失函数：categorical_crossentropy（多分类交叉熵损失）。设计思路：适配多分类任务与one-hot标签，可有效衡量模型预测概率与真实标签的差距，损失值越小，模型预测越准确； 评估指标：accuracy（准确率）。设计思路：直观评估模型分类效果，准确率预测正确的样本数总样本数，便于快速判断模型训练效果。 （2）训练回调函数（优化核心）添加两项关键回调函数，优化训练过程，避免过拟合与无效训练，设计思路如下： EarlyStopping（早停法）：监控验证集损失（val_loss），耐心值（patience）10。设计思路：训练过程中，若验证集损失连续10轮不下降，说明模型已收敛，继续训练会导致无效计算甚至过拟合，此时自动停止训练，并恢复验证集损失最小时的模型权重，确保模型处于最优状态； ModelCheckpoint（模型保存）：监控验证集准确率（val_accuracy），保存最优模型（save_best_onlyTrue）。设计思路：自动保存验证集准确率最高的模型（保存为best_car_direction_model.keras），无需手动记录最优模型，后续可直接加载使用，避免重新训练。 （3）训练参数配置训练时配置批量大小（batch_size）、训练轮数（epochs）、验证集，设计思路如下： batch_size64：适配普通CPU内存，每次训练64张样本，平衡训练速度与内存占用，避免批量过大导致内存不足，批量过小导致训练缓慢； epochs20：设置足够的训练轮数，确保模型有足够的时间学习数据规律，同时配合早停法，无需训练满20轮，避免无效训练； validation_data(X_val, y_val)：训练过程中用验证集实时评估模型效果，根据验证集损失与准确率调整模型权重，避免模型只拟合训练集（过拟合）。 6. 模型评估与结果可视化思路模型评估与可视化的核心目的是“验证模型效果、确认数据质量、分析训练趋势”，确保项目达到预期目标，具体思路如下： （1）模型评估用测试集（未参与训练、未参与验证）评估模型泛化能力，输出测试集损失与准确率，核心判断标准： 测试集损失：接近0，说明模型预测与真实标签差距小； 测试集准确率：≥99%，说明模型已充分学习车辆位置与行驶方向的映射关系，泛化能力强，无明显过拟合。 （2）结果可视化（两项关键可视化，全面验证） 样本验证图：随机选取5张样本图，展示图像、车头方向、行驶方向标签、车辆姿态，核心目的是“确认数据生成无误”——检查车辆姿态与车头方向是否匹配、标签标注是否合理、车辆与车头是否完整（无截断）； 训练曲线：绘制两张曲线（损失曲线、准确率曲线），核心目的是“分析训练趋势”： 损失曲线：训练集损失（train_loss）与验证集损失（val_loss）逐步下降并趋于平稳，且两者差距较小，说明模型收敛、无过拟合； 准确率曲线：训练集准确率（train_accuracy）与验证集准确率（val_accuracy）逐步上升并趋于平稳，且两者差距较小，说明模型学习效果好、泛化能力强。 四、项目完整运行结果以下是项目运行的完整终端输出，包含TensorFlow日志、模型结构、训练过程、最终评估结果，可直接对照自身运行结果验证项目正确性： TextD:\\my project\\cnn-2\\.venv\\Scripts\\python.exe D:\\my project\\cnn-2\\cnn.py 2026-02-05 20:46:32.598293: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2026-02-05 20:46:35.299182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.D:\\my project\\cnn-2\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead. super().__init__(activity_regularizer=activity_regularizer, **kwargs)2026-02-05 20:46:37.019214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.Model: sequential┌─────────────────────────────────┬────────────────────────┬───────────────┐│ Layer (type) │ Output Shape │ Param # │├─────────────────────────────────┼────────────────────────┼───────────────┤│ conv2d (Conv2D) │ (None, 30, 30, 32) │ 320 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ batch_normalization │ (None, 30, 30, 32) │ 128 ││ (BatchNormalization) │ │ │├─────────────────────────────────┼────────────────────────┼───────────────┤│ max_pooling2d (MaxPooling2D) │ (None, 15, 15, 32) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ conv2d_1 (Conv2D) │ (None, 13, 13, 64) │ 18,496 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ batch_normalization_1 │ (None, 13, 13, 64) │ 256 ││ (BatchNormalization) │ │ │├─────────────────────────────────┼────────────────────────┼───────────────┤│ max_pooling2d_1 (MaxPooling2D) │ (None, 6, 6, 64) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ conv2d_2 (Conv2D) │ (None, 4, 4, 128) │ 73,856 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ batch_normalization_2 │ (None, 4, 4, 128) │ 512 ││ (BatchNormalization) │ │ │├─────────────────────────────────┼────────────────────────┼───────────────┤│ max_pooling2d_2 (MaxPooling2D) │ (None, 2, 2, 128) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ flatten (Flatten) │ (None, 512) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ dense (Dense) │ (None, 256) │ 131,328 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ dropout (Dropout) │ (None, 256) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ dense_1 (Dense) │ (None, 64) │ 16,448 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ dropout_1 (Dropout) │ (None, 64) │ 0 │├─────────────────────────────────┼────────────────────────┼───────────────┤│ dense_2 (Dense) │ (None, 5) │ 325 │└─────────────────────────────────┴────────────────────────┴───────────────┘ Total params: 241,669 (944.02 KB) Trainable params: 241,221 (942.27 KB) Non-trainable params: 448 (1.75 KB)Epoch 1/2055/55 ━━━━━━━━━━━━━━━━━━━━ 3s 29ms/step - accuracy: 0.7140 - loss: 1.0157 - val_accuracy: 0.0440 - val_loss: 2.5356Epoch 2/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.8400 - loss: 0.5609 - val_accuracy: 0.0440 - val_loss: 3.5172Epoch 3/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - accuracy: 0.8477 - loss: 0.4715 - val_accuracy: 0.0440 - val_loss: 4.2830Epoch 4/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.8589 - loss: 0.4113 - val_accuracy: 0.0440 - val_loss: 5.1124Epoch 5/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.8694 - loss: 0.3654 - val_accuracy: 0.0490 - val_loss: 5.2911Epoch 6/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.8831 - loss: 0.3105 - val_accuracy: 0.0640 - val_loss: 5.3669Epoch 7/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.8957 - loss: 0.2834 - val_accuracy: 0.0680 - val_loss: 4.8968Epoch 8/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9049 - loss: 0.2518 - val_accuracy: 0.0980 - val_loss: 4.3102Epoch 9/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9171 - loss: 0.2195 - val_accuracy: 0.2380 - val_loss: 2.9597Epoch 10/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9254 - loss: 0.1916 - val_accuracy: 0.4110 - val_loss: 1.7583Epoch 11/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9346 - loss: 0.1729 - val_accuracy: 0.5670 - val_loss: 1.0569Epoch 12/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9463 - loss: 0.1474 - val_accuracy: 0.7920 - val_loss: 0.5172Epoch 13/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9537 - loss: 0.1291 - val_accuracy: 0.9000 - val_loss: 0.2761Epoch 14/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9554 - loss: 0.1206 - val_accuracy: 0.9350 - val_loss: 0.2312Epoch 15/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9631 - loss: 0.1025 - val_accuracy: 0.9790 - val_loss: 0.0968Epoch 16/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9717 - loss: 0.0836 - val_accuracy: 0.9790 - val_loss: 0.0837Epoch 17/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9780 - loss: 0.0694 - val_accuracy: 0.9830 - val_loss: 0.0778Epoch 18/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 25ms/step - accuracy: 0.9806 - loss: 0.0601 - val_accuracy: 0.9900 - val_loss: 0.0548Epoch 19/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9829 - loss: 0.0573 - val_accuracy: 0.9860 - val_loss: 0.0545Epoch 20/2055/55 ━━━━━━━━━━━━━━━━━━━━ 1s 24ms/step - accuracy: 0.9849 - loss: 0.0552 - val_accuracy: 0.9910 - val_loss: 0.0428测试集损失：0.0326测试集准确率：0.9920（99.20%）进程已结束，退出代码为 0 五、运行结果分析与项目总结1. 运行结果详细分析​\t结合上述运行结果，从模型结构、训练过程、最终效果三个维度，详细分析项目运行情况，验证项目是否达到预期目标： （1）模型结构分析模型总计241,669个参数（944.02 KB），属于轻量化模型，符合设计预期： 卷积模块参数占比最高（320+128+18496+256+73856+51293568），核心用于特征提取，参数数量合理，既保证特征提取能力，又避免冗余； 全连接模块参数（131328+16448+325148101），逐步压缩特征维度，参数数量可控，避免全连接层参数过多导致的过拟合与计算量激增； 非训练参数（448）为批量归一化层的参数，不参与训练，仅用于加速收敛，设计合理。 （2）训练过程分析 准确率趋势：训练集准确率从第1轮的71.40%逐步提升至第20轮的98.49%，验证集准确率从0.0440提升至99.10%，两者逐步趋于一致，说明模型学习效果好，泛化能力强； 损失趋势：训练集损失从1.0157逐步下降至0.0552，验证集损失从2.5356下降至0.0428，两者均逐步趋于平稳，且差距较小（训练集0.0552，验证集0.0428），说明模型无过拟合，收敛稳定； 训练速度：每轮训练时间约1-3秒（普通CPU），20轮总计约30秒，符合轻量化模型的设计预期，新手可快速验证训练效果； 回调函数效果：验证集准确率逐步提升，未出现连续10轮不下降的情况，因此模型训练满20轮，最终保存的最优模型准确率可达99.10%（验证集），回调函数起到了有效的优化作用。 （3）最终效果分析​\t测试集损失0.0326，准确率99.20%，远超预期目标（≥99%），说明模型效果优异：","categories":["编程"]},{"title":"简易AI五子棋（q-learning）","path":"/posts/37ac43b8/","content":"简易五子棋AI对战系统：Q-learning与监督学习全程使用Python实现，代码可直接运行。 一、前言基于强化学习和监督学习，训练五子棋对弈AI 二、准备本次项目所需依赖极少，提前通过pip安装核心依赖即可，无需复杂的环境配置： pip install torch numpypip install tqdm` 核心依赖：PyTorch（模型训练推理）、NumPy（数据处理与棋盘维护），tqdm 用于训练过程进度条展示 三、项目整体架构整个项目分为3个核心文件，低耦合、高内聚，每个文件承担明确的职责，方便后续扩展与修改： gomoku_data.py：高质量自对弈样本生成，为CNN模型提供”学习素材”； train_model.py：CNN模型定义与训练，生成可用于对弈的模型文件； main.py：可视化对弈界面与AI核心决策，实现玩家与AI的实时对战。 三者的执行流程为：生成样本（gomoku_data.py） → 训练模型（train_model.py） → 启动对弈（main.py） 四、核心思路讲解第一步：生成高质量自对弈样本核心逻辑 实现基础工具：棋盘状态管理、合法落子校验、胜负判断、连子检测（345连）； 设计优先级落子策略：直接赢棋 阻挡对手赢棋 自己连4 阻挡对手连4 自己连3 相邻位置 随机落子，确保生成的样本带有明确的攻防逻辑； 批量生成对局：运行2000局自对弈，记录每一步的”棋盘状态 + 落子位置”，保存为npz格式（方便PyTorch高效加载）。 关键亮点 样本带有明确的策略性，避免无意义的随机落子，提升模型训练效率； 自动过滤无效对局，确保生成的样本具备可用性； 输出文件gomoku_train_data.npz包含”棋盘状态数组”和”落子索引数组”，直接适配后续模型训练。 第二步：训练CNN五子棋落子预测模型有了样本之后，我们需要训练一个模型，让它从样本中学习”在某个棋盘状态下，应该落子在哪里”。 核心逻辑 定义轻量级CNN模型：输入为3通道15x15棋盘（对应”玩家棋子、AI棋子、空位置”），输出为225个落子位置的得分（15x15225）； 采用两层卷积层提取棋盘空间特征，配合批归一化提升训练稳定性； 采用两层全连接层，将卷积特征映射为落子位置得分； 模型结构简洁，新手易理解，训练速度快，无需高端GPU。 加载自对弈样本：实现自定义GomokuDataset类，解析npz文件，转换为PyTorch可处理的张量； 模型训练配置：采用CrossEntropyLoss损失函数（适配分类任务，落子位置可视为225分类）、Adam优化器（收敛速度快）、学习率调度器（防止训练后期震荡）； 保存训练模型：训练完成后，保存模型参数为gomoku_model.pth，供后续对弈界面加载。 关键亮点 模型结构与后续对弈界面的模型定义完全一致，避免加载时出现”结构不匹配”错误； 训练过程带有进度条展示，实时输出损失值，方便监控训练效果； 支持CPU训练，普通电脑即可完成，门槛低。 第三步：搭建可视化对弈界面与AI核心决策​\t我们通过Tkinter搭建可视化界面，同时实现 模型预测与Q-Learning优化的AI决策逻辑。 Part 1：可视化界面搭建 棋盘绘制：绘制15x15标准五子棋棋盘，包含横线、竖线与星位（3、7、11位置），还原真实五子棋场景； 棋子绘制：区分玩家黑棋与AI白棋，通过鼠标点击事件实现玩家落子，自动校验落子合法性（越界、重复落子）； 游戏控制：实现”开始新游戏””重置棋盘”按钮，支持游戏状态重置； 胜负提示：落子后自动检测横、竖、斜四个方向的五子连珠，弹出提示框告知对局结果。 Part 2：AI核心决策逻辑AI落子优先级严格遵循”先保命、再获胜、后常规”，彻底避免低级失误，具体优先级如下： 优先补自己4连（获胜）：实时检测自身的4连优势，只要检测到，立即补子形成5连，直接获胜 优先堵对手4连（保命）：实时检测对手的4连威胁，只要检测到，立即落子堵塞，防止对手下一步获胜； 优先堵对手3连（防威胁）：检测对手的3连威胁，堵塞其形成4连的可能，遏制对手进攻； 优先补自己3连（造威胁）：检测自身的3连优势，补子形成4连，给对手施加防守压力； 模型+Q-Learning预测（常规决策）：无明显攻防威胁时，加载训练好的CNN模型获取落子得分，结合Q-Learning在线优化落子选择，提升决策灵活性； 随机落子（极端兜底）：无模型文件时，自动降级为随机落子，不影响游戏运行。 关键亮点 通过check_opponent_threat（检测对手威胁）和check_ai_advantage（检测自身优势）两个方法，100%识别3连4连，避免模型训练不确定性带来的低级失误； Q-Learning在线优化：通过Q表记录棋盘状态与落子价值，在线对弈过程中不断更新Q值，提升AI的自适应能力； 兼容性强：无gomoku_model.pth文件时，AI自动降级为硬编码策略落子，依然具备攻防能力，不影响对弈体验； 五、项目运行效果验证 运行gomoku_data.py，生成gomoku_train_data.npz样本文件； 运行train_model.py，训练完成后生成gomoku_model.pth模型文件； 运行main.py，点击”开始新游戏”，即可开始与AI对弈： 玩家落子形成3连4连，AI会立即堵塞，不会漏防； AI形成3连4连后，会优先补子冲5获胜，不会浪费机会； 无明显攻防威胁时，AI会基于CNN模型落子，具备灵活的常规策略。 六、改错历程在项目开发过程中，AI的决策逻辑经历了三次核心迭代，从”无策略乱落子”逐步优化为”攻防兼备、能攻善守”，每一次改错都针对性解决了实际对弈中的关键问题，具体历程如下： 1. 初始问题：AI纯随机落子，强化学习无效问题表现项目初期，仅实现了Q-Learning强化学习框架和基础可视化界面，未加入任何策略逻辑，运行游戏后发现，AI落子完全随机——无论玩家如何落子、是否形成连子威胁，AI都不会针对性防守，也不会主动连子，甚至会落在棋盘边缘无意义的位置，对弈体验极差；同时观察Q-Learning的Q表的更新日志，发现Q值始终处于随机波动状态，并未随着对局次数增加而收敛，强化学习完全没有发挥作用。 排查与错误原因通过逐行调试代码、打印关键变量，最终定位到两个核心错误： 错误1：Q-Learning的状态哈希生成异常，get_state_hash方法中，误用board.tostring()（NumPy新版本已废弃），导致不同棋盘状态生成相同的哈希值，Q表无法正确记录”棋盘状态-落子动作”的对应关系，强化学习无法积累有效经验； 错误2：未设置任何基础落子策略，仅依赖Q-Learning的ε-贪心策略，而初期Q表为空，AI的”探索率”（epsilon0.1）虽低，但无有效Q值可利用，本质上还是随机落子；同时，未加载任何训练好的CNN模型，缺乏基础决策依据，强化学习没有”学习方向”。 修改方案 修复哈希生成问题：将board.tostring()替换为board.tobytes()，适配NumPy新版本，确保每个不同的棋盘状态都能生成唯一的哈希值，让Q表可以正常更新和查询； def get_state_hash(self, board): 将棋盘状态转换为哈希值（用于Q表存储） # 修复前：使用已废弃的tostring()方法 # return hash(board.tostring()) # ❌ 老版本NumPy方法 # 修复后：使用tobytes()方法 return hash(board.tobytes()) # ✅ 新版本NumPy方法 新增基础落子逻辑：在ai_choose_action方法中，先筛选”相邻已有棋子”的合法位置，优先从这些位置中选择落子（避免无意义的边缘落子），给强化学习提供基础方向； 完善Q-Learning参数配置：调整学习率（lr0.1）、折扣因子（gamma0.9），延长Q值的更新周期，确保Q表能随着对局次数增加逐步收敛，让强化学习逐步积累有效落子经验。 修改效果AI不再完全随机落子，会优先选择已有棋子附近的位置，落子更具合理性；Q表能够正常更新，随着对局次数增加，AI会逐步避开无意义的落子位置，强化学习开始发挥作用，但此时AI仍不会判断局势、不会连子和防守，仅解决了”乱落子”的基础问题。 2. 进阶问题：AI优先附近落子，但不会判断局势问题表现解决随机落子问题后，AI能够优先选择已有棋子附近的位置落子，但依然存在明显缺陷——不会判断对局局势：既不会主动形成3连、4连的进攻态势，也不会识别玩家的连子威胁并堵塞；比如玩家连续落子形成3连，AI依然会在无关位置落子，不会针对性防守；AI自身有机会形成3连时，也会错过机会，始终处于”被动跟随”状态，对弈毫无挑战性。 排查与错误原因核心原因是缺乏局势判断逻辑，AI仅能根据”位置是否相邻”选择落子，无法识别”连子威胁”和”连子优势”，具体体现在： 未实现连子检测功能：没有编写count_continuous_pieces（连续棋子计数）、check_opponent_threat（对手威胁检测）等方法，无法识别玩家和AI自身的3连、4连状态； 落子优先级混乱：仅设置了”相邻位置优先”，未定义”防守进攻常规落子”的优先级，AI无法判断”哪个位置更重要”； 强化学习与局势判断脱节：Q-Learning仅能根据历史落子经验优化选择，但无法理解”连子优势””漏堵输棋”的核心规则，经验积累效率极低。 修改方案 新增局势判断核心方法：在BoardManager类中，实现count_continuous_pieces方法，用于统计某一方向上的连续棋子数；新增check_opponent_threat方法，专门检测玩家的3连、4连威胁，为防守策略提供支撑； def count_continuous_pieces(self, x, y, player, dx, dy): 统计某方向上的连续棋子数（含当前位置） count = 0 nx, ny = x, y while 0 = nx self.size and 0 = ny self.size and self.board[nx, ny] == player: count += 1 nx += dx ny += dy return countdef check_opponent_threat(self): 检测对手（玩家）的3连/4连威胁，返回优先级堵塞位置 opponent = 1 four_threats = [] # 对手4连（必堵） three_threats = [] # 对手3连（次必堵） directions = [(0, 1), (1, 0), (1, 1), (1, -1)] for x in range(self.size): for y in range(self.size): if self.board[x, y] == 0: for dx, dy in directions: left_count = self.count_continuous_pieces(x - dx, y - dy, opponent, -dx, -dy) right_count = self.count_continuous_pieces(x + dx, y + dy, opponent, dx, dy) total = left_count + right_count if total == 4: if (x, y) not in four_threats: four_threats.append((x, y)) elif total == 3: left_block = not (0 = x - (left_count + 1)*dx self.size and 0 = y - (left_count + 1)*dy self.size) right_block = not (0 = x + (right_count + 1)*dx self.size and 0 = y + (right_count + 1)*dy self.size) if not (left_block and right_block) and (x, y) not in three_threats: three_threats.append((x, y)) if four_threats: return four_threats[0] elif three_threats: return three_threats[0] return None 优化落子优先级：在ai_choose_action方法中，新增”堵塞对手4连→堵塞对手3连”的防守优先级，优先处理玩家的致命威胁，再选择常规落子位置； 联动强化学习与局势判断：将局势判断的结果融入Q-Learning的奖励机制，若AI成功堵塞玩家的4连，给予正向奖励（reward10）；若AI漏堵玩家的4连导致输棋，给予负向奖励（reward-20），引导强化学习向”防守优先”的方向积累经验。 修改效果AI具备了基础的局势判断能力，能够识别玩家的3连、4连威胁，并优先落子堵塞；落子不再局限于”相邻位置”，会主动选择能遏制玩家进攻的位置，对弈的挑战性显著提升；但此时AI仍存在一个关键缺陷——只会防守，不会主动追求胜利，即便自身形成4连、有机会一步获胜，也会选择防守或常规落子，浪费获胜机会。 3. 最终问题：AI会堵塞你落子，但不会主动胜利问题表现经过前两次修改，AI的防守能力已经基本完善，能够精准堵塞玩家的3连、4连威胁，不会出现”漏堵致命威胁”的低级失误，但新的问题随之出现：AI只会被动防守，不会主动进攻、追求胜利；比如AI自身已经形成4连，只需补子一步就能形成5连获胜，但AI依然会选择堵塞玩家的无关3连，或落在其他无意义的位置；即便没有玩家的威胁，AI也不会主动形成3连、4连，始终处于”被动防守”状态，无法主动结束对局。 排查与错误原因核心原因是缺乏AI自身进攻优势的检测逻辑，落子优先级仅侧重防守，未兼顾进攻，具体体现在： 未检测AI自身的连子优势：仅实现了check_opponent_threat（检测玩家威胁），未编写检测AI自身3连、4连优势的方法，AI无法知晓自己的连子状态，自然不会主动补子冲5； 落子优先级失衡：仅设置了”防守优先级”，未将”AI自身冲5、冲4”纳入高优先级，导致AI即便有获胜机会，也会优先选择防守，忽视进攻； 强化学习奖励机制偏向防守：之前的奖励机制仅针对防守行为给予正向奖励，未对AI主动连子、冲5获胜的行为给予奖励，导致AI缺乏”进攻动力”。 修改方案 新增AI进攻优势检测方法：在BoardManager类中，新增check_ai_advantage方法，与check_opponent_threat对称，专门检测AI自身的3连、4连优势，识别”补子即获胜””补子即形成4连”的关键位置； def check_ai_advantage(self): 检测AI自身的3连/4连优势，返回优先级冲5位置 ai = 2 four_advantages = [] # AI4连（补子即5连获胜） three_advantages = [] # AI3连（补子即4连） directions = [(0, 1), (1, 0), (1, 1), (1, -1)] for x in range(self.size): for y in range(self.size): if self.board[x, y] == 0: for dx, dy in directions: left_count = self.count_continuous_pieces(x - dx, y - dy, ai, -dx, -dy) right_count = self.count_continuous_pieces(x + dx, y + dy, ai, dx, dy) total = left_count + right_count if total == 4: if (x, y) not in four_advantages: four_advantages.append((x, y)) elif total == 3: left_block = not (0 = x - (left_count + 1)*dx self.size and 0 = y - (left_count + 1)*dy self.size) right_block = not (0 = x + (right_count + 1)*dx self.size and 0 = y + (right_count + 1)*dy self.size) if not (left_block and right_block) and (x, y) not in three_advantages: three_advantages.append((x, y)) if four_advantages: return four_advantages[0] elif three_advantages: return three_advantages[0] return None 调整落子优先级（核心修改）：重新定义AI落子优先级，兼顾防守与进攻，最终确定优先级为：堵对手4连（保命）→ 补自己4连（冲5获胜）→ 堵对手3连（防威胁）→ 补自己3连（造威胁）→ 模型+Q-Learning预测 → 随机落子，确保AI在守住致命威胁的前提下，优先追求自身获胜； def ai_choose_action(self): AI落子逻辑：防致命 追获胜 基础策略 # 第一步：优先堵对手4连（保命） opponent_four_threat = self.board_manager.check_opponent_threat() if opponent_four_threat is not None: x, y = opponent_four_threat if self.board_manager.board[x, y] == 0: return x, y # 第二步：优先补自己4连（冲5获胜） ai_four_advantage = self.board_manager.check_ai_advantage() if ai_four_advantage is not None: x, y = ai_four_advantage if self.board_manager.board[x, y] == 0: return x, y # 第三步：堵对手3连（防威胁） opponent_three_threat = self.board_manager.check_opponent_threat() if opponent_three_threat is not None and opponent_three_threat != opponent_four_threat: x, y = opponent_three_threat if self.board_manager.board[x, y] == 0: return x, y # 第四步：补自己3连（造威胁） ai_three_advantage = self.board_manager.check_ai_advantage() if ai_three_advantage is not None and ai_three_advantage != ai_four_advantage: x, y = ai_three_advantage if self.board_manager.board[x, y] == 0: return x, y # 第五步：模型+Q-Learning决策（无攻防威胁时） # ...（后续代码） 优化强化学习奖励机制：新增进攻相关奖励，若AI补自己4连获胜，给予高额正向奖励（reward50）；若AI补自己3连形成4连，给予正向奖励（reward20），引导AI主动进攻、积累进攻经验；同时，加载训练好的CNN模型，让AI在无攻防威胁时，能够基于模型预测选择最优进攻位置。 修改效果AI彻底实现”攻防兼备”：既能精准堵塞玩家的3连、4连威胁，避免自身输棋；也能主动识别自身的3连、4连优势，优先补子冲5获胜、冲4造威胁；当玩家与AI同时有致命威胁时，AI会先堵对手4连（保命），再补自己4连（获胜），符合五子棋的对弈逻辑；强化学习的Q表也能同时积累防守和进攻经验，AI的决策越来越灵活，对弈体验达到预期。 附录：完整项目代码附录1：gomoku_data.py（高质量自对弈样本生成）import numpy as npimport random# 棋盘管理类：负责棋盘状态维护与更新class BoardManager: def __init__(self): self.size = 15 # 3通道棋盘：[0]玩家1（黑）、[1]玩家2（白）、[2]空位置（辅助标识） self.board = np.zeros((self.size, self.size, 3), dtype=np.int8) self.history = [] def update_board(self, x, y, player): 更新棋盘：落子后更新对应通道，记录历史状态 if not (0 = x self.size and 0 = y self.size): return False # 确保当前位置为空 if self.board[x, y, 0] == 1 or self.board[x, y, 1] == 1: return False # 记录历史 self.history.append(np.copy(self.board)) # 重置当前位置所有通道，设置对应玩家通道 self.board[x, y, :] = 0 self.board[x, y, player-1] = 1 self.board[x, y, 2] = 0 # 空位置通道置0 # 补充空位置通道（可选，优化样本特征） for i in range(self.size): for j in range(self.size): if self.board[i, j, 0] == 0 and self.board[i, j, 1] == 0: self.board[i, j, 2] = 1 return True def get_state(self): 获取当前棋盘状态副本 return np.copy(self.board)# 合法落子校验类class Validator: def __init__(self, board_manager): self.board_manager = board_manager self.size = board_manager.size def check_valid(self, x, y): 判断单个位置是否合法（空位置且在棋盘内） if not (0 = x self.size and 0 = y self.size): return False return self.board_manager.board[x, y, 0] == 0 and self.board_manager.board[x, y, 1] == 0 def get_all_valid_positions(self): 获取所有合法落子位置 valid_pos = [] for x in range(self.size): for y in range(self.size): if self.check_valid(x, y): valid_pos.append((x, y)) return valid_pos# 胜负判断函数：检测五子连珠def judge_win(board, x, y, player): size = board.shape[0] directions = [(0, 1), (1, 0), (1, 1), (1, -1)] # 横、竖、正斜、反斜 player_channel = player - 1 for dx, dy in directions: count = 1 # 正向遍历 nx, ny = x + dx, y + dy while 0 = nx size and 0 = ny size and board[nx, ny, player_channel] == 1: count += 1 nx += dx ny += dy # 反向遍历 nx, ny = x - dx, y - dy while 0 = nx size and 0 = ny size and board[nx, ny, player_channel] == 1: count += 1 nx -= dx ny -= dy # 五子连珠判定 if count = 5: return True return False# 连子判断函数：检测n连子（n=3/4）def has_n_pieces(board, x, y, player, n): size = board.shape[0] directions = [(0, 1), (1, 0), (1, 1), (1, -1)] player_channel = player - 1 for dx, dy in directions: count = 1 # 正向遍历 nx, ny = x + dx, y + dy while 0 = nx size and 0 = ny size and board[nx, ny, player_channel] == 1: count += 1 nx += dx ny += dy # 反向遍历 nx, ny = x - dx, y - dy while 0 = nx size and 0 = ny size and board[nx, ny, player_channel] == 1: count += 1 nx -= dx ny -= dy # n连子判定 if count = n: return True return False# 高优先级落子策略：生成带逻辑的自对弈落子def get_better_action(validator, board_manager, current_player): valid_pos = validator.get_all_valid_positions() if not valid_pos: return None, None board = board_manager.get_state() size = board_manager.size opponent = 2 if current_player == 1 else 1 # 定义各优先级落子列表 win_pos = [] # 直接赢棋（5连） block_win_pos = [] # 阻挡对手赢棋 four_pos = [] # 自己连4 block_four_pos = [] # 阻挡对手连4 three_pos = [] # 自己连3 # 遍历所有合法位置，评估落子价值 for (x, y) in valid_pos: # 评估自己落子价值 temp_board_self = board.copy() temp_board_self[x, y, :] = 0 temp_board_self[x, y, current_player-1] = 1 is_self_win = judge_win(temp_board_self, x, y, current_player) has_self_four = has_n_pieces(temp_board_self, x, y, current_player, 4) has_self_three = has_n_pieces(temp_board_self, x, y, current_player, 3) # 评估阻挡对手价值 temp_board_opp = board.copy() temp_board_opp[x, y, :] = 0 temp_board_opp[x, y, opponent-1] = 1 is_opp_win = judge_win(temp_board_opp, x, y, opponent) has_opp_four = has_n_pieces(temp_board_opp, x, y, opponent, 4) # 按优先级归类 if is_self_win: win_pos.append((x, y)) elif is_opp_win: block_win_pos.append((x, y)) elif has_self_four: four_pos.append((x, y)) elif has_opp_four: block_four_pos.append((x, y)) elif has_self_three: three_pos.append((x, y)) # 按优先级选择落子 if win_pos: return random.choice(win_pos) elif block_win_pos: return random.choice(block_win_pos) elif four_pos: return random.choice(four_pos) elif block_four_pos: return random.choice(block_four_pos) elif three_pos: return random.choice(three_pos) # 无连子机会，选相邻位置（提升样本策略性） adjacent_pos = [] for (x, y) in valid_pos: has_adjacent = False for dx in [-1, 0, 1]: for dy in [-1, 0, 1]: if dx == 0 and dy == 0: continue nx, ny = x + dx, y + dy if 0 = nx size and 0 = ny size: if board[nx, ny, 0] == 1 or board[nx, ny, 1] == 1: has_adjacent = True break if has_adjacent: break if has_adjacent: adjacent_pos.append((x, y)) if adjacent_pos: return random.choice(adjacent_pos) else: return random.choice(valid_pos)# 生成自对弈数据主函数def generate_self_play_data(num_games=2000): data = [] for game_idx in range(num_games): # 进度提示 if (game_idx + 1) % 200 == 0: print(f已生成 game_idx+1/num_games 局对局) # 初始化每局游戏 board_manager = BoardManager() validator = Validator(board_manager) current_player = 1 game_over = False while not game_over: valid_pos = validator.get_all_valid_positions() if not valid_pos: # 棋盘下满，平局 break # 获取高优先级落子 x, y = get_better_action(validator, board_manager, current_player) if x is None or y is None: break # 记录样本：落子前状态 + 落子位置索引（x*15 + y） state = board_manager.get_state() action_idx = x * board_manager.size + y data.append((state, action_idx)) # 更新棋盘并判断胜负 board_manager.update_board(x, y, current_player) if judge_win(board_manager.get_state(), x, y, current_player): game_over = True break # 切换玩家 current_player = 2 if current_player == 1 else 1 # 保存数据到npz文件 if len(data) == 0: print(警告：未生成任何训练数据！) return # 转换为numpy数组，适配PyTorch训练 states = np.array([d[0] for d in data], dtype=np.float32) actions = np.array([d[1] for d in data], dtype=np.int64) np.savez(gomoku_train_data.npz, states=states, actions=actions) print(f数据生成完成！共 len(states) 个样本，保存为 gomoku_train_data.npz)if __name__ == __main__: generate_self_play_data(num_games=2000) 附录2：train_model.py（CNN模型训练）import numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import Dataset, DataLoaderfrom torch.optim.lr_scheduler import StepLRfrom tqdm import tqdm# 设备配置：优先使用GPU，无GPU则使用CPUdevice = torch.device(cuda if torch.cuda.is_available() else cpu)# 1. 定义CNN模型（与main.py中模型结构完全一致，避免加载不匹配）class GomokuModel(nn.Module): def __init__(self): super(GomokuModel, self).__init__() # 卷积层：提取棋盘空间特征 self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) self.bn2 = nn.BatchNorm2d(32) # 全连接层：映射为落子位置得分 self.fc1 = nn.Linear(32 * 15 * 15, 128) self.fc2 = nn.Linear(128, 15 * 15) # 输出225个落子位置的得分 # 激活函数与Dropout self.relu = nn.ReLU() self.dropout = nn.Dropout(0.3) def forward(self, x): # 卷积层 + 批归一化 + 激活函数 x = self.relu(self.bn1(self.conv1(x))) x = self.relu(self.bn2(self.conv2(x))) # 展平特征图 x = x.view(-1, 32 * 15 * 15) # 全连接层 + Dropout x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x# 2. 自定义Dataset类：加载自对弈样本class GomokuDataset(Dataset): def __init__(self, data_path): # 加载npz文件 data = np.load(data_path) self.states = data[states] self.actions = data[actions] # 调整数据形状：(N, H, W, C) → (N, C, H, W)（适配PyTorch卷积层输入格式） self.states = np.transpose(self.states, (0, 3, 1, 2)) def __len__(self): 返回样本总数 return len(self.actions) def __getitem__(self, idx): 返回单个样本（状态张量 + 动作标签） state = torch.tensor(self.states[idx], dtype=torch.float32) action = torch.tensor(self.actions[idx], dtype=torch.long) return state, action# 3. 模型训练函数def train_model(data_path=gomoku_train_data.npz, epochs=50, batch_size=64, lr=0.001): # 加载数据集 dataset = GomokuDataset(data_path) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0) # 初始化模型、损失函数、优化器 model = GomokuModel().to(device) criterion = nn.CrossEntropyLoss() # 分类损失函数，适配落子位置预测 optimizer = optim.Adam(model.parameters(), lr=lr) scheduler = StepLR(optimizer, step_size=10, gamma=0.8) # 学习率调度器 # 开始训练 model.train() for epoch in range(epochs): total_loss = 0.0 # 进度条展示训练过程 with tqdm(total=len(dataloader), desc=fEpoch epoch+1/epochs) as pbar: for batch_idx, (states, actions) in enumerate(dataloader): # 数据移至设备（GPU/CPU） states = states.to(device) actions = actions.to(device) # 前向传播 outputs = model(states) loss = criterion(outputs, actions) # 反向传播与优化 optimizer.zero_grad() loss.backward() optimizer.step() # 累计损失值 total_loss += loss.item() pbar.update(1) pbar.set_postfix(batch_loss: loss.item(), avg_loss: total_loss/(batch_idx+1)) # 更新学习率 scheduler.step() # 输出每轮平均损失 avg_loss = total_loss / len(dataloader) print(fEpoch epoch+1 平均损失：avg_loss:.6f) # 保存训练好的模型 torch.save(model.state_dict(), gomoku_model.pth) print(模型训练完成！已保存为 gomoku_model.pth)if __name__ == __main__: train_model(epochs=50, batch_size=64, lr=0.001) 附录3：main.py（可视化对弈与AI核心决策）import tkinter as tk from tkinter import messagebox import numpy as np import torch import torch.nn as nn # ---------------------- 1. 模型定义（与 train_model.py 完全一致，避免加载不匹配）---------------------- class GomokuModel(nn.Module): def __init__(self): super(GomokuModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(16) self.bn2 = nn.BatchNorm2d(32) self.fc1 = nn.Linear(32 * 15 * 15, 128) self.fc2 = nn.Linear(128, 15*15) self.relu = nn.ReLU() self.dropout = nn.Dropout(0.3) def forward(self, x): x = self.relu(self.bn1(self.conv1(x))) x = self.relu(self.bn2(self.conv2(x))) x = x.view(-1, 32 * 15 * 15) x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x # ---------------------- 2. 核心工具类（棋盘管理、攻防检测）---------------------- class BoardManager: def __init__(self, size=15): self.size = size # 棋盘状态：0=空，1=玩家（黑棋），2=AI（白棋） self.board = np.zeros((size, size), dtype=np.int8) # 模型输入用的3通道状态（one-hot编码） self.model_board = np.zeros((3, size, size), dtype=np.float32) def reset(self): 重置棋盘 self.board = np.zeros((self.size, self.size), dtype=np.int8) self.model_board = np.zeros((3, self.size, self.size), dtype=np.float32) def update_board(self, x, y, player): 更新棋盘状态（player=1/2），返回是否更新成功 if 0 = x self.size and 0 = y self.size and self.board[x, y] == 0: self.board[x, y] = player # 更新模型输入的3通道棋盘（one-hot编码） self.model_board = np.zeros((3, self.size, self.size), dtype=np.float32) self.model_board[0][self.board == 1] = 1.0 # 玩家棋子通道 self.model_board[1][self.board == 2] = 1.0 # AI棋子通道 self.model_board[2][self.board == 0] = 1.0 # 空位置通道 return True return False def get_model_input(self): 获取模型输入的张量（增加batch维度，适配PyTorch） input_tensor = torch.from_numpy(self.model_board).unsqueeze(0) return input_tensor def is_win(self, x, y, player): 判断落子后是否获胜（五子连珠） directions = [(0, 1), (1, 0), (1, 1), (1, -1)] # 横、竖、正斜、反斜 for dx, dy in directions: count = 1 # 正向遍历 nx, ny = x + dx, y + dy while 0 = nx self.size and 0 = ny self.size and self.board[nx, ny] == player: count += 1 nx += dx ny += dy # 反向遍历 nx, ny = x - dx, y - dy while 0 = nx self.size and 0 = ny self.size and self.board[nx, ny] == player: count += 1 nx -= dx ny -= dy if count = 5: return True return False def count_continuous_pieces(self, x, y, player, dx, dy): 统计某方向上的连续棋子数（含当前位置） count = 0 nx, ny = x, y while 0 = nx self.size and 0 = ny self.size and self.board[nx, ny] == player: count += 1 nx += dx ny += dy return count def check_opponent_threat(self): 检测对手（玩家）的3连/4连威胁，返回优先级堵塞位置 opponent = 1 four_threats = [] # 对手4连（必堵） three_threats = [] # 对手3连（次必堵） directions = [(0, 1), (1, 0), (1, 1), (1, -1)] for x in range(self.size): for y in range(self.size): if self.board[x, y] == 0: for dx, dy in directions: left_count = self.count_continuous_pieces(x - dx, y - dy, opponent, -dx, -dy) right_count = self.count_continuous_pieces(x + dx, y + dy, opponent, dx, dy) total = left_count + right_count if total == 4: if (x, y) not in four_threats: four_threats.append((x, y)) elif total == 3: left_block = not (0 = x - (left_count + 1)*dx self.size and 0 = y - (left_count + 1)*dy self.size) right_block = not (0 = x + (right_count + 1)*dx self.size and 0 = y + (right_count + 1)*dy self.size) if not (left_block and right_block) and (x, y) not in three_threats: three_threats.append((x, y)) if four_threats: return four_threats[0] elif three_threats: return three_threats[0] return None def check_ai_advantage(self): 检测AI自身的3连/4连优势，返回优先级冲5位置 ai = 2 four_advantages = [] # AI4连（补子即5连获胜） three_advantages = [] # AI3连（补子即4连） directions = [(0, 1), (1, 0), (1, 1), (1, -1)] for x in range(self.size): for y in range(self.size): if self.board[x, y] == 0: for dx, dy in directions: left_count = self.count_continuous_pieces(x - dx, y - dy, ai, -dx, -dy) right_count = self.count_continuous_pieces(x + dx, y + dy, ai, dx, dy) total = left_count + right_count if total == 4: if (x, y) not in four_advantages: four_advantages.append((x, y)) elif total == 3: left_block = not (0 = x - (left_count + 1)*dx self.size and 0 = y - (left_count + 1)*dy self.size) right_block = not (0 = x + (right_count + 1)*dx self.size and 0 = y + (right_count + 1)*dy self.size) if not (left_block and right_block) and (x, y) not in three_advantages: three_advantages.append((x, y)) if four_advantages: return four_advantages[0] elif three_advantages: return three_advantages[0] return None # ---------------------- 3. Q-Learning 优化器（在线优化落子选择）---------------------- class QLearningAgent: def __init__(self, size=15, lr=0.1, gamma=0.9, epsilon=0.1): self.size = size self.lr = lr # 学习率 self.gamma = gamma # 折扣因子 self.epsilon = epsilon # 探索率 self.q_table = # Q表：key=棋盘状态哈希，value=动作Q值 def get_state_hash(self, board): 将棋盘状态转换为哈希值（用于Q表存储） return hash(board.tobytes()) def get_q_value(self, state_hash, action): 获取Q值（不存在则返回0） if state_hash not in self.q_table: self.q_table[state_hash] = np.zeros(self.size * self.size, dtype=np.float32) return self.q_table[state_hash][action] def update_q_value(self, state_hash, action, reward, next_state_hash): 更新Q表（Q-Learning核心公式） current_q = self.get_q_value(state_hash, action) next_max_q = np.max(self.q_table.get(next_state_hash, np.zeros(self.size * self.size))) new_q = current_q + self.lr * (reward + self.gamma * next_max_q - current_q) self.q_table[state_hash][action] = new_q def choose_action(self, model_output, valid_actions): ε-贪心策略：结合模型输出与Q值选择动作 if np.random.random() self.epsilon: # 探索：随机选择合法动作 return np.random.choice(valid_actions) else: # 利用：结合模型得分与Q值选择最优动作 q_values = self.q_table.get(self.get_state_hash(model_output), np.zeros_like(model_output)) combined_scores = model_output + q_values valid_scores = combined_scores[valid_actions] best_idx = np.argmax(valid_scores) return valid_actions[best_idx] # ---------------------- 4. 五子棋GUI界面与主逻辑---------------------- class GomokuGame: def __init__(self, root): self.root = root self.root.title(五子棋（AI对战-攻防兼备）) self.size = 15 self.cell_size = 30 self.canvas_width = self.size * self.cell_size self.canvas_height = self.size * self.cell_size # 初始化核心组件 self.board_manager = BoardManager(self.size) self.q_agent = QLearningAgent(self.size) self.device = torch.device(cuda if torch.cuda.is_available() else cpu) self.model = self.load_model() self.game_running = False # 创建GUI组件 self.create_widgets() def create_widgets(self): 创建GUI界面元素 # 棋盘画布 self.canvas = tk.Canvas( self.root, width=self.canvas_width, height=self.canvas_height, bg=#F0D9B5 ) self.canvas.pack(padx=10, pady=10) self.canvas.bind(Button-1, self.on_click) # 绑定鼠标点击落子事件 # 按钮框架 self.btn_frame = tk.Frame(self.root) self.btn_frame.pack(pady=5) # 开始新游戏按钮 self.start_btn = tk.Button( self.btn_frame, text=开始新游戏, command=self.start_game, width=15 ) self.start_btn.grid(row=0, column=0, padx=5) # 重置棋盘按钮 self.reset_btn = tk.Button( self.btn_frame, text=重置棋盘, command=self.reset_game, width=15 ) self.reset_btn.grid(row=0, column=1, padx=5) # 绘制初始棋盘 self.draw_board() def draw_board(self): 绘制棋盘网格与星位 self.canvas.delete(all) # 绘制横线与竖线 for i in range(self.size): # 横线 self.canvas.create_line( self.cell_size // 2, self.cell_size // 2 + i * self.cell_size, self.canvas_width - self.cell_size // 2, self.cell_size // 2 + i * self.cell_size, fill=#000000 ) # 竖线 self.canvas.create_line( self.cell_size // 2 + i * self.cell_size, self.cell_size // 2, self.cell_size // 2 + i * self.cell_size, self.canvas_height - self.cell_size // 2, fill=#000000 ) # 绘制星位（五子棋标准星位） star_positions = [3, 7, 11] for x in star_positions: for y in star_positions: self.canvas.create_oval( self.cell_size // 2 + x * self.cell_size - 5, self.cell_size // 2 + y * self.cell_size - 5, self.cell_size // 2 + x * self.cell_size + 5, self.cell_size // 2 + y * self.cell_size + 5, fill=#000000 ) def draw_piece(self, x, y, player): 绘制棋子（1=黑棋，2=白棋） center_x = self.cell_size // 2 + x * self.cell_size center_y = self.cell_size // 2 + y * self.cell_size radius = self.cell_size // 2 - 2 if player == 1: # 玩家：黑棋 self.canvas.create_oval( center_x - radius, center_y - radius, center_x + radius, center_y + radius, fill=#000000, outline=#000000 ) else: # AI：白棋 self.canvas.create_oval( center_x - radius, center_y - radius, center_x + radius, center_y + radius, fill=#FFFFFF, outline=#000000 ) def load_model(self): 加载训练好的五子棋模型 model = GomokuModel().to(self.device) try: model.load_state_dict(torch.load(gomoku_model.pth, map_location=self.device)) model.eval() # 切换为评估模式（关闭Dropout） messagebox.showinfo(模型加载, 成功加载训练好的五子棋模型！) except FileNotFoundError: messagebox.showwarning(模型加载, 未找到 gomoku_model.pth，AI将使用策略落子！) model = None except RuntimeError as e: messagebox.showerror(模型加载失败, f模型结构不匹配：str(e)) model = None return model def ai_choose_action(self): AI落子逻辑：防致命 追获胜 基础策略 # 第一步：优先堵对手4连（保命） opponent_four_threat = self.board_manager.check_opponent_threat() if opponent_four_threat is not None: x, y = opponent_four_threat if self.board_manager.board[x, y] == 0: return x, y # 第二步：优先补自己4连（冲5获胜） ai_four_advantage = self.board_manager.check_ai_advantage() if ai_four_advantage is not None: x, y = ai_four_advantage if self.board_manager.board[x, y] == 0: return x, y # 第三步：堵对手3连（防威胁） opponent_three_threat = self.board_manager.check_opponent_threat() if opponent_three_threat is not None and opponent_three_threat != opponent_four_threat: x, y = opponent_three_threat if self.board_manager.board[x, y] == 0: return x, y # 第四步：补自己3连（造威胁） ai_three_advantage = self.board_manager.check_ai_advantage() if ai_three_advantage is not None and ai_three_advantage != ai_four_advantage: x, y = ai_three_advantage if self.board_manager.board[x, y] == 0: return x, y # 第五步：模型+Q-Learning决策（无攻防威胁时） valid_positions = np.argwhere(self.board_manager.board == 0) if len(valid_positions) == 0: return None, None valid_actions = [x * self.size + y for (x, y) in valid_positions] if self.model is not None: with torch.no_grad(): # 关闭梯度计算，提升推理速度 input_tensor = self.board_manager.get_model_input().to(self.device) model_output = self.model(input_tensor).squeeze().cpu().numpy() best_action_idx = self.q_agent.choose_action(model_output, valid_actions) x = best_action_idx // self.size y = best_action_idx % self.size else: # 无模型：随机选择合法位置 x, y = valid_positions[np.random.randint(0, len(valid_positions))] return x, y def on_click(self, event): 鼠标点击事件（玩家落子） if not self.game_running: return # 转换鼠标坐标为棋盘坐标 x = int((event.x - self.cell_size // 2) // self.cell_size) y = int((event.y - self.cell_size // 2) // self.cell_size) # 验证并更新玩家落子 if 0 = x self.size and 0 = y self.size and self.board_manager.board[x, y] == 0: self.board_manager.update_board(x, y, 1) self.draw_piece(x, y, 1) # 判断玩家是否获胜 if self.board_manager.is_win(x, y, 1): messagebox.showinfo(游戏结束, 恭喜你！获胜了！) self.game_running = False return # AI落子（包含攻防兜底策略） ai_x, ai_y = self.ai_choose_action() if ai_x is not None and ai_y is not None: self.board_manager.update_board(ai_x, ai_y, 2) self.draw_piece(ai_x, ai_y, 2) # 判断AI是否获胜 if self.board_manager.is_win(ai_x, ai_y, 2): messagebox.showinfo(游戏结束, AI获胜！再试一次吧！) self.game_running = False return def start_game(self): 开始新游戏 self.reset_game() self.game_running = True messagebox.showinfo(游戏开始, AI会优先堵你的3连/4连，也会优先补自己的3连/4连冲5获胜！) def reset_game(self): 重置游戏 self.board_manager.reset() self.draw_board() self.game_running = False # ---------------------- 5. 程序入口 ---------------------- if __name__ == __main__: root = tk.Tk() game = GomokuGame(root) root.mainloop()","categories":["编程"]},{"title":"提示词工程初步（一）","path":"/posts/f57bf148/","content":"提示词工程初步（一）部署好大模型之后，我们可以用其做一些简单的问答，但如何让回答更接近我们想要的方式，这就需要用到提示词（Prompt）与提示词工程（Prompt Engineering）。通过合理设计、组织提示词，可以显著提升大模型输出的准确性、结构化程度与业务贴合度。本文基于 LangChain 框架，从基础模板到少样本学习，逐步介绍提示词工程的入门实践。 一、最简单的提示词模板：PromptTemplate1. 核心意义 实现固定句式与动态变量分离，避免重复手写相似提示词，提升代码复用性与可维护性。 统一输入格式，降低因提示词写法随意导致的模型输出不稳定问题。 配合 LangChain 的链（Chain）机制，将「模板填充 + 模型调用」封装为一步调用，简化业务代码。 2. 代码实现from langchain_core.prompts import PromptTemplatefrom langchain_demo.my_llm import llm# 定义带变量的模板prompt_template = PromptTemplate.from_template(帮我生成一个简短的，关于topic的报幕词。)# 单独渲染模板（不调用模型，用于调试）# res = prompt_template.invoke(topic: 相声)# print(res)# 构建链：模板 → 大模型chain = prompt_template | llm# 传入变量执行并获取结果resp = chain.invoke(topic: 相声)print(resp) 3. 代码说明 PromptTemplate.from_template：通过字符串快速创建模板，变量名 为动态填充位置。 invoke 方法用于传入参数并渲染完整提示文本。 使用管道符 | 串联模板与模型，是 LangChain 推荐的简洁链式写法。 适用于零样本、结构简单、仅需少量变量替换的常规生成场景。 二、ICL 技术：上下文少样本学习（In-Context Learning）1. 概念与作用ICL 即上下文内学习，也常被称作少样本学习（Few-Shot Learning）。核心思路是： 在提示词中加入若干高质量的「问题 - 答案」示例。 让模型从示例中学习推理逻辑、输出格式、回答风格。 无需微调模型，即可显著提升复杂推理、结构化输出的效果。 适合多跳推理、格式严格约束、领域特定话术规范等零样本难以满足的场景。 在 LangChain 中，通过 FewShotPromptTemplate 实现标准化少样本提示管理。 2. 代码实现from langchain_core.prompts import PromptTemplate, FewShotPromptTemplatefrom langchain_demo.my_llm import llm# 步骤一：提供示例examples = [ question: 穆罕默德·阿里和艾伦·图灵谁活得更久？, answer: 是否需要后续问题：是。后续问题：穆罕默德·阿里去世时多大？中间答案：穆罕默德·阿里去世时74岁。后续问题：艾伦·图灵去世时多大？中间答案：艾伦·图灵去世时41岁。所以最终答案是：穆罕默德·阿里 , question: 乔治·华盛顿的外祖父是谁？, answer: 是否需要后续问题：是。后续问题：乔治·华盛顿的母亲是谁？中间答案：乔治·华盛顿的母亲是玛丽·鲍尔·华盛顿。后续问题：玛丽·鲍尔·华盛顿的父亲是谁？中间答案：玛丽·鲍尔·华盛顿的父亲是约瑟夫·鲍尔。所以最终答案是：约瑟夫·鲍尔 , question: 《大白鲨》和《007：大战皇家赌场》的导演是否来自同一个国家？, answer: 是否需要后续问题：是。后续问题：《大白鲨》的导演是谁？中间答案：《大白鲨》的导演是史蒂文·斯皮尔伯格。后续问题：史蒂文·斯皮尔伯格来自哪里？中间答案：美国。后续问题：《007：大战皇家赌场》的导演是谁？中间答案：《007：大战皇家赌场》的导演是马丁·坎贝尔。后续问题：马丁·坎贝尔来自哪里？中间答案：新西兰。所以最终答案是：否 ]# 步骤二：定义单个示例的渲染模板base_prompt_template = PromptTemplate.from_template(问题:question answer)# 步骤三：创建少样本总模板final_template = FewShotPromptTemplate( examples=examples, # 示例列表 example_prompt=base_prompt_template, # 单条示例格式 suffix=问题:input, # 真实问题拼接在最后 input_variables=[input] # 输入变量名)# 步骤四：构建链并调用chain = final_template | llm# 测试不同问题# resp = chain.invoke(input: 巴伦特朗普的父亲是谁)resp = chain.invoke(input: 中国古代历史上唐朝和宋朝哪个朝代延续时间更长)print(resp） ) 3. 关键参数与使用要点 examples：示例数组，每条为 questionanswer 结构，内容、格式、逻辑必须统一。 example_prompt：控制每一条示例如何渲染成文本，保证所有示例格式一致。 suffix：所有示例之后追加的真实用户问题，模型会参照前面示例的范式回答。 input_variables：声明 suffix 中用到的变量，与模板占位符对应。 示例数量建议 3～5 条为宜，过少学习效果差，过多易超出上下文窗口或引入噪声。 好的，这是为您续写的「三、进阶：聊天场景中使用 ICL（FewShotChatMessagePromptTemplate）」 部分的完整内容，已根据您提供的实际运行结果进行了补充和润色： 三、进阶：聊天场景中使用 ICL（FewShotChatMessagePromptTemplate）前面介绍的是「文本格式」的提示词 ICL，而在实际的大模型应用中，更多是聊天场景（多轮对话、区分用户AI 角色），此时需要使用 LangChain 专为聊天场景设计的 FewShotChatMessagePromptTemplate，它能更好地贴合聊天模型的消息格式（区分 HumanAiSystem 角色），提升对话场景下的 ICL 效果。 1. 核心概念说明 聊天场景的提示词核心是「消息列表」，每条消息都有明确的「角色」（humanaisystem）。 FewShotChatMessagePromptTemplate：专门用于在聊天消息列表中嵌入 ICL 示例，保持示例的角色格式与真实对话一致。 MessagesPlaceholder：消息占位符，用于动态填充后续的真实对话消息（支持多轮对话扩展）。 ChatPromptTemplate：聊天场景的提示词模板，与普通 PromptTemplate 的区别是基于「消息角色」构建。 2. 代码实现from langchain_core.messages import HumanMessagefrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import MessagesPlaceholder, FewShotChatMessagePromptTemplatefrom langchain_demo.my_llm import llmfrom langchain_core.prompts import ChatPromptTemplate# 步骤一：准备聊天场景的 ICL 示例（键值对格式，对应后续单条消息模板的变量）examples = [ input: 2 ？ 3 = 5,output: 5, input: 3 ？ 4 = 7,output: 7,]# 步骤二：定义单个示例的聊天消息模板（区分 Human/Ai 角色，对应示例的 input/output）base_prompt = ChatPromptTemplate.from_messages( [ # 每条示例中，用户输入（human 角色）对应 input 变量 (human, input), # 每条示例中，AI 回复（ai 角色）对应 output 变量 (ai, output), ])# 步骤三：创建聊天场景的少样本提示词模板（整合所有示例）few_shot_prompt = FewShotChatMessagePromptTemplate( examples=examples, # 传入聊天场景的示例列表 example_prompt=base_prompt, # 传入单条示例的聊天消息模板)# 步骤四：构建完整的聊天提示词模板（包含系统消息、ICL 示例、真实消息占位符）final_template = ChatPromptTemplate.from_messages([ # 系统消息：定义 AI 的角色和行为准则 (system, 你是一个AI助手！), # 嵌入 ICL 少样本示例（会自动按 base_prompt 的格式渲染所有示例） few_shot_prompt, # 消息占位符：用于动态接收真实的用户对话消息（支持多轮消息传入） MessagesPlaceholder(msgs)])# 步骤五：构建执行链chain = final_template | llm# 步骤六：传入真实用户消息，调用执行链resp = chain.invoke(msgs: [HumanMessage(content=2？9结果是多少)])# 打印结果print(resp) 3. 代码关键解析 ChatPromptTemplate.from_messages()：接收消息列表构建聊天模板，每条消息是 (角色类型， 消息内容) 元组，核心角色包括 system（定义AI行为）、human（用户输入）、ai（AI回复）。 FewShotChatMessagePromptTemplate：聊天场景专属ICL工具，将示例列表按 example_prompt 定义的「Human→Ai」格式渲染，形成完整示例对话链。 MessagesPlaceholder(msgs)：动态消息容器，调用时可传入单个多个 HumanMessageAIMessage，支持多轮对话扩展，无需重构模板。 示例中的 input 是用户提问格式，output 是对应标准答案，模型会从示例中学习隐藏规律（此例中是「数字相加」）。 4. 实际运行结果运行命令输出D:\\my project ew_ai_model_env\\.venv\\Scripts\\python.exe D:\\my project\\LangchainProject\\langchain_demo\\提示词模板-4.py D:\\my project ew_ai_model_env\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isnt compatible with Python 3.14 or greater. from pydantic.v1.fields import FieldInfo as FieldInfoV1content=对于 2 ？ 9，根据前面的例子，我观察到？操作似乎是将两个数字相加。2 ？ 3 = 5 (即 2 + 3 = 5)3 ？ 4 = 7 (即 3 + 4 = 7)按照这个规律，2 ？ 9 的结果应该是:2 + 9 = 11所以，2 ？ 9 = 11additional_kwargs=refusal: Noneresponse_metadata=token_usage: completion_tokens: 144, prompt_tokens: 84, total_tokens: 228, completion_tokens_details: accepted_prediction_tokens: None, audio_tokens: 0, reasoning_tokens: 0, rejected_prediction_tokens: None, text_tokens: 0, prompt_tokens_details: audio_tokens: 0, cached_tokens: 0, text_tokens: 0, image_tokens: 0, input_tokens: 0, output_tokens: 0, input_tokens_details: None, claude_cache_creation_5_m_tokens: 0, claude_cache_creation_1_h_tokens: 0, model_provider: openai, model_name: claude-3-7-sonnet-20250219, system_fingerprint: None, id: msg_bdrk_013v2a4K1AkZZUY4hnLbh2Nk, finish_reason: stop, logprobs: Noneid=lc_run--019c185a-cca9-7581-a0c3-77edef4787db-0tool_calls=[]invalid_tool_calls=[]usage_metadata=input_tokens: 84, output_tokens: 144, total_tokens: 228, input_token_details: audio: 0, cache_read: 0, output_token_details: audio: 0, reasoning: 0进程已结束，退出代码为 0 结果解读 忽略弃用警告：开头的 Pydantic V1 弃用警告是 LangChain 框架的版本兼容性提示，不影响程序功能运行，可以暂时忽略。 ICL 学习成功：模型成功从两条示例 2 ？ 3 = 5 和 3 ？ 4 = 7 中，学习到了「？ 代表将两个数字相加」的隐藏规律，并正确推导出 2？9 的结果是 11。 输出格式说明：打印的 resp 对象是 LangChain 聊天模型返回的原生 AIMessage 对象，它包含多个属性： content：核心回复内容，即模型推理出的纯文本答案。 additional_kwargs、response_metadata、usage_metadata：这些是请求的元数据，包含了本次调用的令牌消耗详情、使用的模型信息（Claude 3.7 Sonnet）、请求ID等。 优化输出（可选）：如果您仅需要模型回复的纯文本内容（即 content），可以在执行链末尾添加 StrOutputParser() 解析器： chain = final_template | llm | StrOutputParser() 返回的结果变为 如果我们观察前面的例子：2 ？ 3 = 53 ？ 4 = 7我可以推断出这个？操作是将两个数字相加。所以：2 ？ 9 = 2 + 9 = 11答案是11。 通义千问等主流聊天模型的原生交互格式（SystemHumanAssistant 消息流），能有效提升指令遵循和回复的准确性。","categories":["编程"]},{"title":"大模型的私有化部署（一）","path":"/posts/fa00b5a1/","content":"大模型的私有化部署（一）一、部署前期准备1.1 服务器租赁（核心前提）由于个人电脑的算力、显存等硬件条件受限，不推荐本地部署大模型，优先选择云服务器提供的GPU租赁服务，主流平台可选择阿里云、AutoDL等。 选择建议（按场景划分） 测试学习用途：首推 AutoDL 优势：价格低廉，按小时计费，预装完整深度学习环境（无需手动配置Python、Torch等），开箱即用，对新手友好。 生产长期运行用途：优先选择 阿里云、腾讯云 优势：稳定性更强，服务保障更完善，支持弹性扩容，适合企业级落地场景。 1.2 服务器实例创建与信息留存 配置选择：推荐显卡型号 RTX 30904090 或 A100，显存≥16GB（保障8B模型正常运行），按需选择CPU与硬盘容量（建议硬盘≥50GB，用于存储模型文件）。 实例创建完成后，务必妥善记录以下连接信息（丢失后难以找回）： SSH 地址（格式示例：123.456.78.90:12345） 登录密码（或密钥文件，若选择密钥登录） 1.3 PyCharm Pro 远程SSH连接（高效开发必备）为了简化后续的代码编写、文件管理与命令执行，推荐使用PyCharm专业版的远程开发功能，直接连接云服务器进行操作。 详细操作步骤 打开 PyCharm Professional，创建或打开你的项目。 进入路径：File → Settings → Project: [你的项目名] → Python Interpreter。 点击解释器右侧的 Add Interpreter 按钮，选择 **On SSH...**（远程SSH解释器）。 在弹出的配置窗口中，填写服务器信息： Host：服务器公网IP地址（无需带端口） Port：SSH端口号（默认22，AutoDL等平台为随机分配端口，需对应记录的信息） Username：默认通常为 root Password：填写记录的服务器登录密码 后续步骤一路点击 Next，默认选择服务器上的Python解释器即可，完成配置。 验证：配置成功后，PyCharm的终端（Terminal）执行命令会直接在远程服务器上运行，文件操作也会同步到服务器对应目录。 二、模型下载与环境配置2.1 大模型下载（以Qwen3-8B-Instruct为例）我们选择从ModelScope（魔搭社区）下载字节跳动开源的Qwen3-8B-Instruct模型，该模型性能均衡，适合新手入门私有化部署。 步骤1：找到模型官方页面https://www.modelscope.cn/models/qwen/Qwen3-8B-Instruct/summary 步骤2：Python SDK 下载（推荐，自动处理依赖与文件整合） 若未安装ModelScope库，先在PyCharm远程终端执行安装命令：pip install modelscope 创建Python脚本（或直接在终端运行），执行以下下载代码：from modelscope import snapshot_download# 配置模型名称与下载目录# 注意：AutoDL平台大容量数据盘默认路径为 /root/autodl-tmp，优先选择该目录节省系统盘空间model_name = Qwen/Qwen3-8B-Instructsave_dir = /root/autodl-tmp/models# 开始下载模型（首次下载约15GB，耗时取决于网络速度）model_dir = snapshot_download(model_name, cache_dir=save_dir)# 打印模型最终存储路径（后续启动服务需用到，建议复制留存）print(f模型下载完成，存储路径：model_dir) 2.2 安装 vLLM 推理引擎（高速推理核心）vLLM 是一款高性能的大模型推理与服务库，相比原生推理框架，具有显存利用率高、生成速度快、支持高并发的优势，是私有化部署的首选推理引擎。 在PyCharm远程终端中执行以下安装命令： pip install vllm 验证安装成功安装完成后，执行以下命令检查是否安装成功： pip list | grep vllm 若终端返回 vllm 对应的版本号，说明安装完成。 三、启动 OpenAI 兼容 API 服务3.1 启动命令（复制即可使用，需修改模型路径）在PyCharm远程终端中，执行以下命令启动OpenAI兼容的API服务（命令行换行用 \\ 分隔，确保完整复制）： python -m vllm.entrypoints.openai.api_server \\ --model /root/autodl-tmp/models/Qwen/Qwen3-8B-Instruct \\ --served-model-name qwen3-8b \\ --max-model-len 8k \\ --host 0.0.0.0 \\ --port 6006 \\ --dtype bfloat16 \\ --gpu-memory-utilization 0.8 \\ --enable-auto-tool-choice \\ --tool-call-parser hermes 核心参数说明（必看，方便后续自定义配置） 参数 作用说明 --model 模型本地绝对路径（需与下载完成后的实际路径一致，关键！） --served-model-name 对外暴露的模型名称，API调用时需指定该名称 --max-model-len 8k 模型支持的最大上下文长度，Qwen3-8B-Instruct支持最高8k --host 0.0.0.0 允许外部设备访问该服务（本地电脑、其他服务器），不可修改为127.0.0.1 --port 6006 服务端口，可自定义（如8080、9090），需确保端口未被占用且已放行 --dtype bfloat16 采用bfloat16精度运行，平衡推理速度与模型效果，同时节省大量显存 --gpu-memory-utilization 0.8 GPU内存利用率上限（80%），可根据显存大小调整（如显存不足可改为0.7） 启动成功标识若终端输出 INFO: Uvicorn running on http://0.0.0.0:6006，说明服务已成功启动，保持终端窗口开启（关闭终端会终止服务）。 3.2 API 服务测试（验证部署效果）服务启动后，通过以下两种测试方式验证是否可正常调用。 测试1：基础模型列表查询在本地电脑终端（或服务器新终端窗口）执行以下 curl 命令： curl http://你的服务器IP:6006/v1/models 替换 你的服务器IP 为实际公网IP，若返回包含 id: qwen3-8b 的JSON格式数据，说明服务运行正常。 测试2：对话功能测试（实际调用模型生成回复）在本地电脑终端执行以下 curl 命令，发送对话请求： curl http://你的服务器IP:6006/v1/chat/completions \\ -H Content-Type: application/json \\ -d model: qwen3-8b, messages: [ role: user, content: 你好，请用一句话介绍你自己。 ] 若返回包含模型回复内容的JSON数据，说明对话功能正常，私有化部署API服务搭建完成。 四、常见问题与避坑指南 端口不通，无法访问服务 排查步骤：① 确认服务器安全组防火墙已放行 6006 端口（AutoDL在实例控制台「端口映射」中配置）；② 确认启动命令中 --host 为 0.0.0.0；③ 确认服务器IP填写正确，无端口号输入错误。 显存不足（OOM错误），启动失败 解决方案：① 降低 --gpu-memory-utilization 参数（如改为0.7或0.6）；② 将 --dtype bfloat16 改为 --dtype float16（进一步节省显存，少量损失效果）；③ 租用显存更大的GPU实例（如A100 40GB）。 关闭终端后，服务停止运行 解决方案：① 使用 nohup 命令后台运行（命令前加 nohup，结尾加 ，日志保存至 nohup.out）；② 安装 tmuxscreen 工具，创建会话保持服务运行（适合长期部署）。 本地代码调用私有模型 核心配置：将OpenAI SDK的 api_base 改为 http://你的服务器IP:6006/v1，model 改为 qwen3-8b，即可像调用OpenAI官方API一样使用私有模型。","categories":["编程"]},{"title":"杀戮尖塔设计拆解——世界观篇","path":"/posts/de29066e/","content":"《杀戮尖塔》设计拆解——世界观篇 特别鸣谢：​\t贴吧大佬“奇怪的哔哩”的世界观分析贴，本文的分析基于笔者自己的游玩体验和大佬对细节的解析。有兴趣的小伙伴可以在B站观看视频版： ​\t【【万字解析】《杀戮尖塔》铁甲战士背景故事分析】 https://www.bilibili.com/video/BV18cFreWE2S/?share_source=copy_webvd_source=507b3c58ff11ec91bc194db7b0cf2b55 ​\t【还在迷信故障机器人？其实观者早就登神！：【观者】背景故事分析】 https://www.bilibili.com/video/BV1FcPdexE1N/?share_source=copy_webvd_source=507b3c58ff11ec91bc194db7b0cf2b55 ​\t【以凡人之姿狩猎不朽之物：【静默猎手】背景故事分析】 https://www.bilibili.com/video/BV13cP1eyEcG/?share_source=copy_webvd_source=507b3c58ff11ec91bc194db7b0cf2b55 ​\t【超越所有人工制品的无上至尊：【故障机器人】背景故事分析】 https://www.bilibili.com/video/BV1FgN1eaED2/?share_source=copy_webvd_source=507b3c58ff11ec91bc194db7b0cf2b55 一、核心设计目标：用世界观构建“意义感”​\t一个优秀的游戏往往精心构建一套完整且逻辑自洽的世界观，它不仅为游戏提供了可信的叙事根基与行动动机，更在无形中引导玩家自然地沉浸于虚拟情境，深刻强化角色代入感与情感联结，最终使游戏体验超越单纯的玩法交互，成为一段富有意义、令人信服且回味悠长的沉浸式叙事旅程。 1.1 为什么需要有一个世界观​\t《杀戮尖塔》在设计之初就面临Roguelike品类的固有痛点：重复体验容易疲劳、多职业容易同质化、数值成长缺乏情感支撑、玩家决策缺乏意义感。传统的解决方案往往是增加更多随机内容或调整数值平衡，但《杀戮尖塔》选择了更根本的解法——将叙事深度嵌入核心系统。 flowchart TD B{杀戮尖塔解决方案} B --> D[叙事系统化设计] D --> E[每次爬塔都是角色个人史诗] D --> F[每个职业都是完整叙事原型] D --> G[构筑过程就是角色命运塑造] D --> H[每次选择都是角色性格体现] 1.2 世界观带来了什么？在《杀戮尖塔》中，玩家的每一个游戏决策都被赋予了双重意义。例如选择战士职业时，当玩家选择一张“腐化”卡牌时，这不仅是选择了“牺牲防御换取爆发”的游戏策略，更是选择了让角色“向恶魔力量屈服”的叙事路径。这种设计创造了独特的决策循环： 选择卡牌遗物 → 理解叙事意义 → 增强沉浸体验 → 影响后续选择 二、世界观系统化设计框架2.1 角色设计：叙事作为差异化骨架2.1.1 战士：从堕落走向超越的复仇者​\t战士的的核心矛盾在于力量与代价的永恒博弈，是向恶魔低头，还是超越自我？ 力量进化四阶段： 凡人之力阶段： 白卡武装、完美打击、旋风斩，效果扎实且朴实无华，展现战士一开始作为一个普通的军人，他所拥有的扎实的战斗素养； 恶魔之力阶段： 战士向恶魔献祭灵魂，获取更加强大的力量。祭品是战士开始献祭自己（在游戏里具体表现为支付生命值为代价，回能与抽牌）黑暗之拥表现了战士开始拥抱黑暗的一面（给战士强大的过牌能力），腐化代表战士抛弃自己的过往，将灵魂献与恶魔以获得强大的力量（所有技能牌变0费且消耗），恶魔形态更是直接展示战士恶堕成魔的可能性； 灵魂之力阶段： 重振精神，岿然不动，断魂斩等牌，表现战士对光明与自我的发现，抛弃与恶魔的交易，而是去寻求自我救赎与自我超越的可能； 融合之力阶段： 通关尖塔后，结局的紫色火焰象征对恶魔之力的最终掌控，不再受恶魔影响，完成了新生与超越 graph LR A[凡人之力红色，军事武艺] --> B[恶魔之力红→黑，契约堕落] B --> C[灵魂之力蓝色，自我觉醒] C --> D[融合之力紫色，新生超越] style A fill:#ffcccc style B fill:#333333,color:#ffffff style C fill:#ccccff style D fill:#cc99ff 2.1.2 静默猎手：以技艺对抗超凡的智者猎手的设计抛出了一个核心命题：凡人能否凭借智慧与技艺对抗超凡存在？ 这个角色的所有机制都服务于这一叙事主题。 机制与叙事的精准对应： 丢弃机制：“专注要害，摒弃杂念”的猎手思维 → 游戏中的过滤牌库策略 杂技、本能反应等牌，作为抽牌卡，卡面的描述与插画无一不展现猎手的灵敏，在游戏里体现的是超越其他职业的大过牌；而丢弃这一概念表现猎人对于战斗的专注，摒弃杂念，而在游戏中则体现在“本能反应（弃置此牌抽3张）”和“战术大师（弃置此牌回2费）”这两张弃牌体系的核心卡。 小刀流：“高频低伤，技巧至上”的战斗哲学 → 频率乘区的构筑思路 和其余职业的强大设定不同，猎人是部族中的佼佼者，但毕竟只是凡人之躯，为了对抗敌人，猎人需要极致的战斗技巧。刀刃之舞（获得四把0费的小刀）配合能力牌精准（小刀伤害增加）和遗物袖箭（0费攻击牌伤害增加），正是展现了猎人这一特点—— 尽管没有特别的力量（小刀的初始伤害不高），但长期的训练让猎人的准度增加（表现为精准），搭配武器袖箭，也能打出爆炸的伤害。 毒药系统：“智慧弥补力量不足”的生存策略 → 层数积累的伤害方式 毒贼则展示猎手的另一面：缜密的头脑与精密如化学实验般的战术思维，擅长以巧取胜、以智克力。例如“涂毒”（每次攻击附带一点毒）这张能力卡，使毒素般在敌人体内缓慢累积、持续生效。而待到毒至临界时，“催化剂”（毒层数翻至三倍）卡牌的加入，更让毒性如连锁反应般骤然爆发，这正是猎手在实战中精于计算、耐心布局，最终一击制敌的思维体现——她不靠蛮力碾压，而是在最恰当的时机引爆最剧烈的反应。这种战斗风格，既是对“猎人”之名的真正诠释，也是对“智慧胜于力量”这一核心叙事的完美续写。 幽魂形态：“向猎物学习超凡”的成长路径 → 获得无敌防御的能力 这是猎人最可怕之处，她会从对手那里学到对方最引以为傲的优点。四大形态中也属猎手学会的幽魂形态最为强大和泛用。 2.1.3 故障机器人：文明遗产的继承者在杀戮尖塔的世界观里，故障机器人代表了先古之民科技与魔法结合的巅峰成就，其设计围绕“指数成长”和“文明继承”两个核心主题展开。由于笔者对机器人的理解不甚精通，文本也未仔细研究，因此只做简要概述： journey title 机器人的三阶段成长体验 section 启动期 古代造物苏醒: 5 能力牌延迟收益: 3 “需要耐心布局”: 2 section 成长期 系统权限获取: 5 集中力乘法效应: 4 “开始产生质变”: 4 section 巅峰期 继承文明遗产: 5 无限球/递归循环: 5 “我是天灾”: 5 2.1.4 观者：从凡躯到神域的通神者观者的设计展现了游戏中最完整的成长阶梯，从凡人的战斗技巧到触及神域的通神能力，每一步都有清晰的叙事支撑。 力量成长的四个阶梯： 凡人之躯： ​\t疾风连击、粉碎关节、洞见等牌，是观者作为一位战斗者所拥有的基本素质，它们在游戏中也大多表现为白卡； 心灵之力： ​\t姿态切换、情绪化为力量。愤怒让观者的输出能力翻倍，平静则给观者提供能量。其实后者有点符合东方的禅修哲学； 规则操控： ​\t保留机制、现实批准。观者是4位中最接近所谓“神”的存在，她能够保留力量（在游戏里体现为卡牌），等到合适的时机崽使用；她在游戏中有操纵现实（给新加入的卡升级）和确立基础（所有保留的卡减费）这两张卡，体现了她自己超脱于现实的力量。 神域触及： ​\t真言积累、神格形态。神格体系从文本设计上倒是有趣，敬拜、祈祷、五体投地这几张卡积累一定真言后进入神格，象征观者的虔诚，在一步步朝圣中获得神力；而渎神这张卡设计的很有趣，立即进入神格，但下回合会死亡，这展现观者对在高塔中逐渐发现真相，对所谓’‘神“失去信仰。结局里观者睁开紫金色的眼，暗示观者可能依然登神。 2.2 进程设计：三层结构的叙事节奏2.2.1 精心编排的三层结构《杀戮尖塔》的高塔三层不仅是难度递进，更是叙事节奏的精心编排。让难度逐层递进的逻辑更加合理，也更易于玩家接受 第一层：废墟与奠基 叙事氛围：初探遗迹，相对安全，遇到的只有咔咔、毛毛虫，小地精之类的小怪 设计目标：教学与构筑奠基 精英引导：小红（要求攻击浓度）、三柱（要求AOE能力） 第二层：城市与压力 叙事氛围：深入文明腹地，危险加剧，你将会遇到古文明留下的机器造物，仍在塔内居住的盗贼，奴隶贩子等危险人群 设计目标：多维压力测试 压力来源：血量压力、状态压力、牌库污染、经济压力 第三层：时空与检验 叙事氛围：规则崩坏，接近核心，你会遇到巨大的倒下的头颅，信仰蛇这一图腾的蛇女，以及各种奇形怪状的怪物 设计目标：构筑完整性检验 BOSS考官：每个BOSS检验牌组的不同方面 2.2.2 心脏战：终极叙事高潮心脏作为最终BOSS，不仅是难度顶峰，更是叙事主题的集大成者。其四项机制各自对应不同的检验维度和叙事隐喻： graph TD subgraph A[心脏机制设计] B[律动机制] --> C[强制要求防御浓度隐喻：力量的反噬] D[坚不可摧] --> E[杜绝速杀可能隐喻：真正的耐力考验] F[状态污染] --> G[检验牌库纯净度隐喻：混乱中的清醒] H[高成长] --> I[设置软回合限制隐喻：时间的压力] end subgraph J[设计目的] K[全面验收牌组攻防运转清状态全面达标] end A --> J 2.3 随机系统：碎片叙事的引导力杀戮尖塔的叙事从来不是连贯的，它往往在遗物文本，卡面插画和问号事件中，让玩家一步一步拼凑事件的真相 2.3.1 事件系统的叙事化重构传统Roguelike的事件往往是“收益vs损失”的简单博弈。《杀戮尖塔》将这种博弈升级为 “碎片化叙事探索”与“即时资源安全”的复合式抉择。 其设计精髓在于：游戏从不直接讲述完整故事，而是将世界观的线索与角色的过去，拆解为 “可交互的叙事碎片”，散落在玩家必经的决策之中。以“感知石”事件为例： 表层博弈：选项A（损失生命，获得一张随机无色牌） vs 选项B（无事发生，保留血量） 叙事层博弈：选项A（触摸石头，被迫直面一段痛苦或关键的过往记忆，并以一张“卡牌”的形式将这份记忆固化为己用） vs 选项B（拒绝回忆，保持现状，但可能永远错过理解“我是谁”与“我为何在此”的机会） 这种设计将一次普通的资源交换，转化为一次 “角色身份建构” 的主动选择。玩家在衡量生命值与一张未知卡牌的价值时，潜意识里也在回答：“我的角色是否渴望面对真相？他她是一个勇于揭开伤疤的探寻者，还是一个回避过去的生存者？” 2.3.2 碎片化叙事的载体：遗物、卡牌与事件的文本密语 遗物的描述文本：短短一两句话，既是功能说明，也是微型故事。例如“痛楚印记”（战士专属遗物）的描述：“北方部族的战士们用这个来将痛楚转化为力量。”这句话未展开任何背景，却直接点明了战士的出身（北方部族）及其核心战斗哲学（转化痛楚），并与其“伤口”机制形成互文。 卡牌的图像与名称：卡面美术与名称是未被言说的叙事。观者的“诸神之黄昏”卡面描绘雷霆与洪水袭击高塔，其名称直接借用北欧神话中的末日之战，两者结合，强烈暗示了高塔世界曾遭逢一场神罚级的灾难。玩家无需阅读任何文本，便能感受到画面中蕴含的宏大悲剧与毁灭意象。 事件中的对话与选项：随机事件是动态的叙事碎片。在“幽灵议会”事件中，幽灵们会称玩家为“奥涅的玩偶”，并邀请你“尝试我们的力量”。这短短几句对话，至少抛出了三个叙事钩子：奥涅是谁？我们与奥涅是何关系？幽灵的力量本质是什么？玩家通过选择是否接受力量，不仅获得了游戏性的“灵体”卡牌，也亲自介入了这段模糊的亡灵往事，其选择本身就成了对角色与幽灵关系的一种诠释。 载体 叙事内容举例 叙事特点 玩家行为 遗物描述 “捕梦网：北方部族的猎手用它来滤除噩梦，只留下预兆。” 冰山一角，暗示文化、习俗与信仰。 阅读、联想、拼凑族群背景。 卡牌图像名 卡牌“恶魔形态”上战士狰狞的样貌与手中的火焰；“死亡收割”中汲取的蓝色灵魂能量。 视觉叙事，展现角色状态、力量来源或世界片影。 观察、解读图像背后的故事与变化。 事件文本 “女鬼”事件中，那个不断询问你是否认识她的哀伤灵魂。 沉浸式片段，提供情感冲击与未解之谜。 做出选择，参与并定义这段小型叙事。 这种 “碎片化-载体化” 的叙事策略，创造了独特的玩家体验：追求剧情的玩家会像考古学家一样，仔细审视每一件遗物的说明，端详每一张卡牌的画作，品味每一段事件的对话，从中剥离信息，在脑海中构建属于自己的世界观图景。而不关注剧情的玩家，则可以完全无视这些文本与暗示，毫不影响他们享受构筑与爬塔的核心乐趣。叙事由此成为一种 “可选的深度” ，而非强制的灌输，这正是《杀戮尖塔》在叙事系统化设计上的高明之处。 2.4 核心循环：从玩法到意义的升华《杀戮尖塔》的核心循环设计实现了从表层玩法到深层意义的自然过渡： 表层循环（玩家直观感受）：战斗 → 获取奖励 → 选择路线 → 继续战斗 深层循环（叙事赋予意义）：发掘记忆（获取卡牌遗物） → 理解过去（阅读背景文本） → 塑造自我（确定构筑方向） → 面对真相（挑战关键首领） → 改变世界（达成角色结局） 三、设计哲学提炼3.1 四大核心理念理念一：玩法即叙事《杀戮尖塔》彻底摒弃了独立的剧情动画和过场CG。角色的每一个能力、敌人的每一个行为、玩家的每一个抉择，这些本身就是故事的讲述。当战士打出“黑暗之拥”时，这不只是在获得过牌能力，更是在经历被恶魔侵蚀的过程；当猎手使用“丢弃”时，这不只是在过滤牌库，更是在战斗中摒除杂念、专注要害。 理念二：系统作为隐喻游戏中的抽象机制都通过叙事获得了具体的隐喻意义： 弃牌 专注（猎手的战斗哲学） 烧牌 牺牲（战士的力量代价） 姿态切换 心绪流转（观者的心灵控制） 充能球 古代能源（机器人的文明遗产） 理念三：碎片引导想象游戏不提供完整的世界观编年史，而是提供“先古之民”、“荒疫”、“神罚”等关键词和散落线索。这种设计激发了玩家的推理欲和社区讨论，玩家在论坛中拼凑线索、分享理论，极大延长了游戏的生命周期。 理念四：一致性创造沉浸从职业机制到卡牌美术，从遗物效果到敌人设计，从地图氛围到音乐音效，《杀戮尖塔》的所有元素都服务于同一套黑暗、神秘、略带克苏鲁气息的叙事调性。这种高度的一致性创造了强烈的沉浸感。 四、对同类项目的启示4.1 完整的设计路径对于希望借鉴《杀戮尖塔》叙事系统化设计的项目，可以参考以下完整路径： 阶段一：立项阶段的主题选择在为游戏核心玩法寻找叙事主题时，要选择可以深度系统化的主题。例如： 卡牌构筑玩法 → 考古主题（卡牌文物，构筑修复） 资源管理玩法 → 生态主题（资源物种，管理平衡） 角色成长玩法 → 教育主题（升级学习，技能知识） 阶段二：角色设计的叙事整合职业的叙事背景必须是其核心机制的合理解释与情感包装。机制应该是叙事背景的自然延伸，叙事应该是机制的情感化包装。 阶段三：进程设计的主题匹配游戏进程的每个阶段都应有明确的叙事主题，并且这个主题应该体现在该阶段的机制挑战中。如果某一章的主题是“背叛”，那么机制可以围绕“资源逆转”、“盟友变敌人”等设计。 阶段四：内容投放的策略性将剧情信息拆解为可被玩法系统承载的“道具”——卡牌描述、遗物文本、事件选项、敌人台词等。让追求剧情的玩家可以主动挖掘，让不感兴趣的玩家可以完全忽略，实现叙事的可选择消费。 总结：叙事作为系统设计的范式价值《杀戮尖塔》的世界观设计证明，叙事可以不只是游戏的“装饰”或“背景板”，而是可以成为游戏的结构性框架。这套“叙事系统化”的设计方法，创造了几个关键价值： 1. 解决了Roguelike的重复性问题通过让每次爬塔都成为独特的角色史诗，玩家感受到的不是“又一遍相同的流程”，而是“一个新角色的新故事”。 2. 提升了决策的情感重量玩家的每一个游戏决策都同时是叙事决策，这让简单的资源权衡变成了具有情感重量的性格定义。 3. 创造了深度的沉浸感当游戏的所有元素——机制、美术、音效、文本——都服务于同一套叙事调性时，玩家会自然地进入游戏世界。 4. 延长了游戏生命周期碎片化叙事激发了社区的讨论和理论构建，玩家在游戏之外继续探索世界观，形成了良性的生态循环。 graph TD A[传统设计玩法与叙事分离] --> B[玩家体验割裂沉浸感有限] C[杀戮尖塔设计叙事成为系统框架] --> D{四大价值} D --> E[解决重复性问题] D --> F[提升决策重量] D --> G[创造深度沉浸] D --> H[延长生命周期] style C fill:#ccffcc style D fill:#ffffcc 《杀戮尖塔》的成功不仅仅是玩法设计的成功，更是叙事系统化设计范式的成功。它展示了一条将世界观深度融入核心系统的新路径，为整个游戏设计领域提供了宝贵的启示：当叙事不再只是被“讲出来”，而是被“玩出来”时，游戏就能创造出真正独特而深刻的情感体验。 到这里我对杀戮尖塔的拆解工作便告一段落啦，大家如果有什么想法可以在评论区交流哦","categories":["游戏"]},{"title":"杀戮尖塔设计拆解——内容篇","path":"/posts/98ac3117/","content":"《杀戮尖塔》设计内容深度拆解写在前面《杀戮尖塔》以其精妙的规则融合，定义了Roguelike卡牌。其成功远非简单的玩法叠加。本文将以 “内容拆解 → 平衡性分析 → 设计哲学提炼” 的三步框架，由表及里地剖析其设计精髓，揭示这款游戏如何将简单的规则编织成近乎无限的可能，并提供持久的策略乐趣。 第一步：内容拆解——游戏的建筑逻辑任何深度分析都始于对构成要素的系统性理解。 1. 核心驱动循环每一局游戏都由一个简洁有力的引擎驱动，其核心循环如下图所示，它构成了单局游戏最基本的行为骨架： flowchart TD A[战斗消耗生命，检验牌组] --> B[获取奖励三选一卡牌/金币/遗物] B --> C{地图路径决策风险评估与资源规划} C -->|选择路线| A 这个循环持续进行，玩家的每一次选择都在动态塑造着独一无二的牌组与资源状况。 生命值在此循环中扮演着双重角色：它既是需要保障的生存需求，也是可以谨慎花费以换取更高奖励的货币。 （特定事件可以用血量换取收益——遗物或者卡牌） 2. 核心构建维度玩家力量的成长围绕三个可累积的系统展开： 卡牌：主动的操作单元，分为 攻击牌：直接伤害； 技能牌：格挡、施加状态等； 能力牌：整场持续的被动效果； 遗物：提供从数值加成到机制颠覆的各类效果，是构筑产生质变的关键。 数值加成：金刚杵加1力量，光滑的石头加1敏捷。这类遗物提供简单粗暴的加数值； 机制改变：冰淇淋让回合结束后能量不清空，金字塔让回合结束时的手牌不丢弃。这类遗物改变构筑思路和出牌逻辑； 局外成长：古钱币直接给300块，烟斗让火堆额外多了删牌事件。这些遗物让玩家在战斗之外改变构筑提升战力； 药水：一次性的战术储备，用于应急或弥补构筑短板。 提供抽牌，回血，格挡，换牌等效果； 小怪概率掉落，商店稳定购买，事件可能获取； 通过减少药水栏位限制高进阶玩家； 3. 核心博弈资源除了牌组，玩家还需持续管理两种抽象资源： 生命值：如上所述，是驱动核心循环的关键资源，对血量的判断是否足以打精英怪换取遗物； 金币：集中于商店系统的经济资源。用于购买关键卡牌、遗物，以及萌新可能忽视的删卡； 4. 庞大的内容池游戏提供了丰富的交互可能性，确保每次冒险的独特性，每个职业拥有完全独立的卡池与核心机制，玩法迥异。 四大职业： 战士：我认为，战士的最核心机制是消耗： 通过点烧（坚毅+，燃烧契约）和大烧（重振精神，恶魔之焰）完成对局内卡组的精简，进一步打出小循环或无限循环。 常见cambo有剑耸无限（有日晷真无限，带放血伪无限），亮剑妙计，撑振肚皮，进化无谋契约… 除此之外，战士还有以壁垒巩固为核心的防战体系和以力量组件为核心的力量战体系（偏门的有强依赖单卡和火堆的灼热战体系） 猎人：猎人核心是运转，通过摸弃完成过牌回费和零费输出： 弃牌体系对猎人来说是最成熟的体系，组件也最常见，例如最强白卡过牌杂技1费抽4弃1，因为对猎人来说，弃牌往往带来的是正面收益（本能反应被弃后抽牌，战术大师被弃后回费）强大的运转能力使得猎人几乎适配所有0费输出，尤其是金卡华丽收场，0费AOE高伤害，即使打出需要严苛的条件，也值得猎人为此特化构筑 猎人第二分支是毒猎，由于高进阶慢启动吃战损、怕人工等缺点，在没有关键卡催化剂的情况下，往往作为过渡的选择 小刀猎：萌新最爱玩的流派之一，但是最好祈祷不要遇到小刀猎最严厉的父亲哦（会遇到时间吞噬者！） 故障机器人（球重编程）：​\t由于本人的机器人水平并不高，遂邀请友人对其评价，由于内容较长，放至附录1。 观者：观者核心是姿态转换： 愤怒：双倍伤害，但受伤也双倍，观者的最核心机制没有之一。所有观者的输出卡设计时候都要考虑到双倍伤害这一条件。哪怕是初始的打击+1费打9，到观者手上就是一费18，产生质变。在前两层里，愤怒加两个打击足以秒杀许多小怪了。愤怒的机制让观者有了速杀的能力。 平静：退出平静姿态时候回两费。如果说单看愤怒机制，还可以说是风险与机遇并存——高伤害的同时防御压力也翻倍，无法斩杀的怪物就要考虑防守。但是如果加上平静则完全不同，观者完全可以进入暴怒输出打完后在进入平静，不仅不用承受双倍防御压力，还能为下一轮的输出积攒费用。1费平静卡的存在导致观者完全可以在红蓝之间切换的费用不会损失，只要抽牌量足够，观者就能把所有的牌打出去。我相信很多小伙伴打出的第一个无限就是猛虎下山加暴怒不惧妖邪（鸡煲：我要学猛虎下山！）然后释怀的笑 神格：存在感很低的体系，被红蓝遮掩了锋芒，其本身底子不错，有过牌有保留有0费卡，但是组件过于绑定，拿一张意味着就要拿一个体系，观者又是一个对卡组大小很敏感的职业，导致实战很多人不愿意拿第一张神格卡（完全是累赘），进而不碰整个神格体系。就像我在数值篇分析的那样，观者的卡牌数值结构就是存在明显的问题，无用卡占比比其余三个职业加起来都多，这是设计里比较可惜的点。 预见：最意义不明的体系，设计的像一个半成品，不予评价 渐进式挑战：​\t三层逐级攀升的冒险，每层结尾设有机制独特的强力首领，对牌组的完成度提出硬性考验。 一层：发育为主，高难挑战较少，提前准备对策卡即可。如精英小红要求卡组的攻击牌浓度，精英三柱要求卡组中有对群特攻或者快速的点杀能力。总体来说危险程度不大，以引导玩家合理选卡为主。 二层：危险度高，考验卡组抗压能力。高进阶的二层是玩家失败最多的层数。一方面是二层小怪的高攻击欲望带来的血线压力，且常会给玩家加debuff；另一方面玩家卡组大多数在二层尚未成型，如果一层boss遗物掉落的不好的话在二层刚进门遇到双小偷或者3鸟都会比较吃力。且二层精英怪三奴隶和小手攻击欲望很高每回合都有高额攻击，会往卡组里塞伤口，较为克制吃启动和运转的卡组。在二层，玩家得精细把控血量和发育的平衡，既不能横死道中，也不可火堆全睡亏了发育。 三层：卡组成型的检验，引导玩家补强或找到短板。有趣的是，虽然三层小怪的强度高于二层，但玩家压力缺普遍低于二层，三层的实际压力是塔顶的双重boss（进阶20才有）。能过二层，说明玩家卡组已经初步成型，在三层的的主要目的变成了寻找关键key牌补强，或者敲牌删牌发育。而三层boss才是三层的灵魂： 1.觉醒者：特殊机制：2条命，玩家每开一个能力牌觉醒者加两点力量。克制能力多的职业。（在启动了！） 2.时间吞噬者（老头）：特殊机制：玩家每出12牌强制结束回合。特别克制以来运转的体系和低质量无限 3.八体和甜甜圈：特殊机制：高成长，塞眩晕，还有三层人工不好上debuff。 最终boss：心脏。最难的boss。特殊机制如下： 1.律动：每出1张牌受到伤害，伤害量随回合数逐渐递增。这个机制强制要求卡组必须有起防能力，不能打没有防御的无限：例如双剑柄日晷，必须搭配起防卡或者遗物。 2.坚不可摧：每受到200点伤害后，将免疫一切伤害。杜绝了一切速杀的可能，再强大的攻杀卡组，也得至少四回合，变相提高了对卡组中防御浓度的需求。 3.第一回合塞状态牌和挂debuff。作为集大成者，心脏第一回合不会攻击，而是转为向你的抽牌堆里加入五张状态牌，并且给你挂易伤（受到伤害增加），虚弱（造成伤害减少），脆弱（获得格挡减少）等debuff。这对卡组的请状态牌能力提出高要求，并且也干扰很多脆弱的无限和循环手段。 4.高成长：心脏每三回合进行一次强化，提高力量并重置自身负面状态。基本八到九回合之后，心脏的攻击九很难用一般起防手段防御住了。这个机制要求玩家的卡组有在上述负面条件影响下依旧能快速解决战斗的能力。 随机事件： 提供高风险高回报的特殊抉择，持续增加游戏进程的变数。部分事件具有一锤定音的效果，例如二层的灵体事件可以解决绝大多数卡组的启动压力，三层的敲全部牌事件是缺敲位猎人的救命稻草（即使代价是无法回复血量）。由于诸多随机事件的设计，给杀戮尖塔的路线选择带来更加丰富的可能性，也提供了未知的乐趣和博弈性。 第二步：平衡性分析——精密的调控艺术真正让这些模块焕发生机的，是使其相互咬合、充满权衡的精妙平衡设计。下图概括了游戏在几个关键维度上如何设置平衡点： flowchart LR subgraph A [风险与回报的量化] direction LR A1[生命值] -- 作为决策货币 --> A2[计算精英战收益评估事件代价] end subgraph B [资源与污染的博弈] direction LR B1[添加新卡牌] -- 可能 --> B2[稀释牌库浓度] B2 -- 提升 --> B3[删除基础牌的战略价值] end subgraph C [遗物的催化效应] direction LR C1[特定遗物] -- 与卡牌协同 --> C2[引发质变如：蛇眼+高费攻击] C2 -- 引导 --> C3[整局游戏构筑方向] end A --> D[终极目标：在动态约束中规划最优解] B --> D C --> D 1. 风险与回报的量化体系游戏将绝大多数决策转化为可评估的风险与回报。 生命值的隐形标价：一场普通战斗的预期损耗、一个事件选项的潜在代价、挑战精英所需的“入场费”，都经过了精确计算。精英战斗虽然危险，但其掉落的核心遗物价值被设定为“通常值得且经常必要”，尤其是在高层进阶中，回避精英往往意味着后期强度不足。 路线选择的数学博弈：地图设计保证了高收益节点（精英、商店）往往伴随更高的战斗密度或路径成本。玩家需要根据实时状态（血量、金币、牌组关键需求）进行动态规划，计算最优的数学期望。 2. 卡牌强度与“卡组污染”的对抗单卡强度必须置于牌组整体循环效率的全局中考量。 稀释惩罚：盲目添加低质量或不合流的卡牌，会显著降低抽到核心组件的概率。这迫使玩家在“拿牌解决当下问题”与“保持牌组精简专注”之间持续权衡。 删除机制的战略价值：因此，商店中的“删牌”服务成为核心战略环节。移除初始的打击防御以提升卡组中关键牌的浓度，是构筑从粗糙走向精炼的关键一步，这赋予了经济资源（金币）深远的战略意义。 3. 职业核心机制的数值边界​\t具体分析见上文杀戮尖塔的数值分析篇，这里仅作简单介绍。 战士的格挡与力量：格挡值每回合清空，使得壁垒这类保留格挡的卡牌价值凸显。力量成长虽线性，但与多段攻击配合能产生指数级伤害，其潜力被严格限制在高费用或稀有卡中。 猎手的弃牌运转：其核心在于通过大量抽牌、弃牌触发效果（如“本能反应”、“杂技”）和0费卡牌来实现每回合的高额操作量。平衡的关键在于，减少卡组内非关键牌的浓度（初始的打击防御，仅用于过渡的攻击卡），和需要配合的资源卡（战术大师，弃置此牌获得能量）。 故障机器人的集中：作为提升所有充能球效率的全局乘数，获取集中力的手段（如“碎片整理”、“偏差认知”） 观者的姿态切换：“愤怒”姿态下造成与承受伤害均翻倍，这一简洁设计创造了极高的操作风险与爆发潜力，迫使玩家精确规划姿态切换的节奏，实现高效的攻防循环。 4. 遗物的催化剂效应遗物设计的高明之处在于，它们常常是能促使卡组产生化学反的催化剂。 协同引爆点：如死灵之书（每回合第一张二费及以上攻击牌打出两次）与高费攻击牌的配合，能将单次爆发伤害倍增。这类遗物与特定卡牌结合后，便能引发构筑的质变。 构筑导向性：早期获得一个核心遗物（如“异蛇之眼”：每回合多抽二，你卡牌的能力消耗将会随机改变），往往会直接定义整局游戏的构筑方向（多抓高费牌，少抓0费）。这种因势利导的动态规划，正是游戏策略深度的核心体现。 第三步：设计哲学提炼——成功的底层逻辑在具体的平衡性之上，是支撑整个游戏体验的、更为普适的设计理念。这些理念之间的支撑关系，构成了游戏稳固的设计三角： graph TD subgraph 表层体验 D[动态构建的乐趣“现在我能走哪条路？”] end subgraph 底层支柱 A[“限制”创造深度能量/手牌/生命值] --> D B[信息透明下的风险计算将运气转化为策略] --> D C[可控的随机性用选择引导随机结果] --> D end D --> E[核心体验：在约束中感受掌控感与成长的权力] 1. 限制是深度策略的源泉游戏中最引人入胜的决策，几乎都源于与各种限制的对抗。 能量限制（每回合3点）：要求权衡每张卡牌的费用与效果。 手牌与抽牌限制：需要管理牌序并构建过牌引擎。 药水槽位限制：让每个药水选择都意义重大。 生命值限制：将生存压力转化为贯穿全局的决策维度。 正是这些限制的存在，使得那些能够突破限制的效果（获得额外能量、额外抽牌、复制卡牌、无视消耗等）成为玩家构建的终极追求，创造了强烈的成长感与构筑成就感。 2. 信息透明下的风险计算游戏摒弃了战斗中的隐藏骰子，并清晰预告敌人的下回合行动。这巧妙地将运气问题转化为了风险计算问题。玩家思考的不再是会不会出暴击？，而是以我当前的血量和牌库，接下这招后存活并反制的概率有多大？决策基于已知信息与概率估算，成功带来的成就感源于精妙的规划与计算，而非单纯的随机恩赐。 3. 动态适应，而非静态搭配与预先组好固定套牌的传统卡牌游戏不同，《杀戮尖塔》的核心乐趣在于根据已获得的资源，即时规划最优的胜利路径。每一局，玩家都在回答一个动态问题：“基于我当前的卡牌和遗物，我最有可能通往胜利的构筑方向是什么？”这就要求设计必须提供大量中等强度、具备多种协同潜力的卡牌与遗物，确保无论随机获得何种奖励，玩家总能找到一个有发展潜力的方向，而非依赖少数几个固定的通关公式。 4. 可控的随机与正向的雪球效应游戏的随机性被高度结构化的“玩家选择”所包裹与引导。 三选一奖励：允许玩家在随机选项中挑选最符合当前策略的拼图。 地图路线选择：允许玩家根据自己的状态主动趋利避害。 商店购买：允许玩家用通用资源（金币）精确填补短板。 这种设计让玩家感觉自己是在不断将随机性引导向利于自己的方向。一旦关键协同形成，构筑开始正向循环（产生雪球效应），玩家能清晰感受到自己对局面的控制力从弱到强的增长过程。这种不断增强的掌控感，是游戏提供持久正反馈和成瘾性的关键。 结语通过对《杀戮尖塔》“内容-平衡-设计”三层结构的剖析，我们可以看到，其伟大之处在于构建了一个规则简洁但交互深奥的决策模拟器。它以限制创造张力，以透明保障公平，以动态构建鼓励智慧适应，再以可控的随机性提供无穷的探索空间。它将所有游戏元素编织进一张精密的网中，让每一次抽牌、每一次路线选择都充满意义。最终，它提供了一种纯粹而迷人的策略乐趣：在规则与随机的双重约束下，运用智慧，从无到有地构建出一个能够克服万难、独特而强大的系统。这或许就是《杀戮尖塔》能够超越类型，成为设计典范的根本原因。 附录1特别鸣谢：Songhan Cai先生对故障机器人理论提出的大力支持。他是出色鸡煲高手（），让我们看看他对故障机器人的理解： ​\t故障机器人是杀戮尖塔中的最强的角色（括号均为黄逸飞插话：并非），它拥有其他职业难以望其项背的数值和令所有怪物闻风丧胆的机制（数值在哪？）。 ​\t机器人的大部分体系围绕充能球来展开，四种充能球各司其职，在各个端口为机器人构建了全面并且强大（？）的体系支架。 ​\t1.电球是机器人最容易获取的球，他每回合能提供基础稳定的伤害，产球成本低使得能更好在循环中把其他球推出去，在电动力学的加持下还能转变为高贵的aoe，更为可贵的是电球随机选择目标攻击，这让机器人这个职业天生比其他人多了很多很多随机数，能在无数次sl中找到唯一获胜的世界线。 ​\t2.黑球有着更高的爆发伤害，其蓄力机制能很好和循环递归，双放这种牌配合，利用好瞄准生命值最低目标的机制，找到合适的时机选择推出去还是攒球。 ​\t3.等离子白球的获取成本更高，但是其收益十分之恐怖，大多数boss遗物每回合加一费都要让你付出高昂的代价，而这仅仅需要我们鸡煲挂一个白球，同时白球的激发能让机器人马上获得两费，简直是无与伦比的爆发能力。 ​\t4.而最强最强最强的则是冰球，其提供的护甲看似很少，但是在集中，循环和更多球位的多重杠杆之下，冰球会为机器人筑起固若金汤的防御端，而且是每回合自动的。有了集中冰甚至攻击端能仅仅凭一个沙漏就能腐乳时间吞噬者，你只需要每回合按e，然后拿起手机刷十分钟视频。 ​\t现在通过球和集中来分析机器人的攻击和防御端，电球黑球提供伤害，冰球提供防御，那么怎么让他们的数值达到合格水平以应对怪物呢。第一个想到的就是生成更多的球，把原来的球推出去以获得更多数值，这点通过打出电击，冷头，漆黑等优质产球牌就可以做到。第二是通过增加球位，或者通过循环来提升我们每回合球的挂机收益。最重要的第三点，则是提升我们的集中，集中能够同时提升球的被动收益和推球的收益，作为一个独立的乘区增幅机器人的战斗力，能提供集中的牌和遗物都是受机器人所喜爱（尽孝）。无论在局外抓卡还是局内操作，牢记这三点的均衡对机器人来说都是基本且必要的。 ​\t接下来说一下物理攻防，球的数值不受力量，敏捷，易伤，虚弱等等多重常见状态影响，导致其思路有所不同，和某些遗物的配合也较差。但这不意味鸡煲不需要传统攻防牌，在卡组没有成型前，我们需要适当抓取一些物理攻防过渡牌，如眼部，光束射线，自动护盾，这是职业特性和怪物环境双重决定的（严父小红），有些牌同时兼顾物理和产球，如球闪，冰川，愁云，这让他们前中后期都有不俗的发挥。 ​\t而物理机则是机器人一种完全不同的思路，以重编程为绝对核心，用掉集中这个对于机器人来说能废掉一半牌的代价换取配合比起其他职业少的多的力敏，是比灼热战和点穴观更垃圾的存在，所以绝大时候机器人还是以物理为辅助，充能球为核心。（在这里我插一嘴，其他职业的棱镜玩法中，重编程是一张非常强力的卡牌，可惜跟了机器人） ​\t来到运转端，机器人在运转方面的优劣势都十分明显，他不像战士能烧干所有牌打小循环，不能学猎人娴熟通过猴戏弃牌打出复杂操作，更不是某个凭借着崩坏的数值就无法无天洁癖抓牌嗯凑唐氏红蓝无限的最弱角色（别逗你观姐笑了）能碰瓷的。大部分鸡煲都是均卡思路，好牌多抓，通过打出能力牌逐步建立优势，但是打能力牌也需要抽牌位和费用且收益延迟，故而启动稍稍慢那么一点点也就是鸡煲稍有瑕疵的小缺点了（偏差认知了，偏差认知，偏差认，偏差，偏…）。而优点是，在费用方面鸡煲手握着最多的优质加费，如四职业唯一白卡费用牌内核，简单粗暴的双倍能量，一边烧牌一边赚费的回收。过牌方面，无论是没数值的快检还是有数值的编冲，都在一个可用的中庸水平，能够定向检索和复用的全息搜寻万物也都是十分适配均卡体系的。但缺少一锤定音的过牌核心如黑拥猛虎一直是鸡煲的一个痛点，定位相似的散热片并不是那么好用（我要学猛虎下山！）。","categories":["游戏"]},{"title":"杀戮尖塔设计拆解——数值篇","path":"/posts/eff9c8a5/","content":"《杀戮尖塔》数值设计深度拆解​ 在卡牌与Roguelike融合的游戏品类中，《杀戮尖塔》的成功离不开其精妙且恰到好处的数值设计。作为一款策略类，以决策为核心乐趣的游戏，它不依靠华丽的特效或复杂的剧情，而是通过能量、卡牌、遗物、难度等多维度的数值平衡，构建了“战斗-奖励-提升”的数值循环与“费用-收益”匹配的价值曲线，最终形成既充满随机性又具备可控策略空间的体验闭环。 ​ 我累计500余小时的攀登历程中，深刻感受到每一处数值细节都暗藏策划巧思，下文将结合参考文章的权威数值体系，从核心资源价值、卡牌数值架构、遗物联动逻辑、难度平衡设计四个核心维度，拆解其数值设计的精髓。 ⚡ 一、核心资源数值（能量）：作为抉择基石​ 《杀戮尖塔》的战斗数值体系中，每回合的能量是贯穿始终的核心约束变量，其数值设定直接决定了策略决策的深度，且存在明确的价值量化标准。游戏初始默认单回合能量上限为3点（部分职遗物可提升），能量存在固定价值曲线：0点能量基础价值为3，之后每增加1点能量额外提供2点价值，这一量化标准构成了卡牌设计的核心基准，而非简单的“3点约束”。这一数值体系实则构建了“攻击-防御-能量-运转”的四维策略平衡，迫使玩家在每回合进行精准的资源分配与机会成本权衡。 🔹 1.1 从设计意图来看​ 初始3点能量的数值设定是价值曲线与策略深度的最优解：若能量过多，玩家可同时触发多个高价值连携效果，策略抉择的价值会大幅降低；若能量过少，则无法支撑基础连携的触发，破坏卡组运转流畅度。3点能量恰好让玩家每回合需在基础输入+单一乘区或者多乘区组合但基础输入弱化等策略中抉择。 ​ 面对一层精英敌人乐嘉（额这是外号，具体名字不记得了QAQ）即将到来的高额攻击，玩家需在1费防御（5格挡，价值匹配）+2费攻击（打击1费打6）与2费防御（初始防御1费5格挡）+1费运转（抽牌调整手牌）之间权衡，而这种抉择直接关联生命值资源的消耗与留存，正是数值驱动策略乐趣的核心来源。 ​ 值得注意的是，即使是0费卡牌也存在隐性成本，其占用的抽牌位会导致错过其他关键卡牌，这一机会成本同样被纳入能量价值体系的考量范畴。 🔹 1.2 能量数值的弹性设计进一步丰富了策略维度​ 游戏通过融合之锤（boss遗物）孙子兵法（普通遗物）等遗物提供能量加成，例如孙子兵法的设定是：如果不使用攻击卡下回合+1能量，本质是通过策略选择换取能量价值的提升（额外获得2点基础价值）；而许多boss遗物通过支付某些特定代价（例如锤子：火堆无法升级牌；绿帽：无法获得金币）换取每回合稳定多一费。这种代价支付加稳定收益的数值设计，既保证了体系的稳定性，又为流派构建提供了多样化可能。 🃏 二、卡牌数值体系（费效比）：精准定位与价值平衡​ 卡牌是玩家实现策略意图的核心载体，《杀戮尖塔》的卡牌数值设计遵循：卡组定位、费效比、联动组件三大原则，核心逻辑是以能量价值为基准，构建‘基础输入-乘区放大-导出转化’的强度公式，确保不同类型、不同费用的卡牌在性价比上保持平衡，同时尽可能避免策略单一化，掩盖其他玩法的问题。单张卡牌效果普遍简洁，仅提供基础输入或单一乘区，需通过组合形成强大连携，这既降低了玩家记忆成本，又提升了策略深度。 ⚠️ 2.1 插一句设计缺陷​ （上一段所说的避免策略单一化只在前两个职业，即战士和猎人上完成的较为完美，而我们鸡煲，呃呃，懂得都懂，问就是还在启动，在启动。观者的设计则是严重不平衡，暴怒带来的双倍伤害让其很多职业卡的费效比不尽人意，红蓝无限的玩法带来的收益又过于超模，掩盖了观者的神格预见点穴等体系。虽说这几个体系本来也有数值不足的问题。只能说观者的出现，对玩家来说可能是玩起来比较容易爽的角色，但从数值设计的角度看，无疑是失败的） 🔹 2.2 在攻击卡牌设计中​ 数值严格匹配能量价值曲线，且通过“基础输入+乘区”的组合形成差异化定位。 ​ 1费的打击基础伤害为6点（价值2，匹配1费能量价值5中的基础部分），2费的重击伤害为14点且附带3倍力量加成（价值5+2，2费能量价值7完全匹配），而非简单的倍数关系；0费的愤怒可造成6点伤害并在弃牌堆放入一张自身，收益为2+1（额外留存卡牌资源），精准匹配0费能量3点的基础价值。 ​ 不同卡牌的乘区特性适配不同流派：战士的重击依赖力量乘区实现爆发输出，静默猎手的0费小刀则通过多次攻击乘区触发精准，袖箭等被动效果，成为小刀流的核心输入组件。这种设计让低费卡并非弱卡，而是适配高频连击流派，高费卡则需放弃当回合其他资源，形成明确的机会成本权衡。 🔹 2.3 防御卡牌的数值设计同样遵循价值曲线，且聚焦临时防护​ 游戏将格挡设定为临时性防御资源，没有特殊卡牌和遗物的情况下防御不会保留，1点格挡等价于1点生命值。 ​ 1费的初始卡防御提供5点格挡（价值匹配1费能量），2费的稀有卡岿然不动提供30点格挡，但是是消耗卡，不能够反复利用。玩家可根据敌方伤害数值选择合适的防御卡牌：面对小额持续伤害，优先使用低费防御卡节省能量；面对大额单次伤害，则需投入高费防御卡或组合多张防御卡。 ​ 更关键的是，防御维度可通过特定卡牌转化为攻击强度，例如战士的肚皮（全身撞击，未升级是1费，升级后变成0费）可造成当前格挡值的攻击伤害，将防御资源转化为输出输入，这一设计打破了维度壁垒，让“高防御+高输出”的防战流派成为可能。而能力卡壁垒可以保留格挡值的设定，更是强化了防御乘区的效果，让格挡资源可跨回合累积，催生格挡转伤害的防御流派。 🔹 2.4 卡牌升级的数值提升也严格匹配价值均衡原则​ 打击升级后伤害从6提升至9（提升幅度50%），防御升级后格挡从5提升至8（提升幅度60%），升级后的价值仍与能量费用精准匹配，既让升级后的卡牌具备明显优势，又不会让未升级卡牌完全失效。这一设计确保玩家在升级卡牌与回复生命的火堆处选择中产生策略博弈——是提升长期战力以减少后续生命值消耗，还是保证当前生存以规避即时风险？这种博弈让每一次火堆选择都成为影响数值循环的关键决策。 🧩 三、遗物数值联动：打破基础规则的策略裂变引擎​ 遗物系统是《杀戮尖塔》数值设计的点睛之笔，其核心价值在于通过“数值加成+机制改变”的组合，优化卡组的体系，催生多样化的流派构建。遗物的数值设计并非简单的数值堆砌，而是精准匹配卡牌的连携逻辑，同时服务于“战斗-奖励-提升”的数值循环（因为遗物通常由精英怪掉落，而打精英怪可能面临生存危险，你说是吧鸡煲TvT（还在启动，在启动，启动，启…）），与卡牌体系形成约束和强化的联动闭环——没有合适的卡牌组合，遗物的数值优势无法发挥；缺少遗物的数值加持，卡牌组合也难以形成质变。 🔹 3.1 普通遗物：补充短板和优化体验为主​ 战士的初始遗物“燃烧之血”效果是战斗后回血6点，解决了前期回血资源稀缺的问题，让玩家可更激进地挑战战斗；普通遗物背包的效果是第一轮多抽2张牌，提升关键牌上手率，降低第一轮的鬼抽概率；还有商店遗物工具箱，棱镜，前者可以每场战斗3选1一张无色牌，就冲着能选到神话和打钱手这也是必买遗物，而棱镜则打破职业卡限制，让选择里多出其他职业的职业卡，可能会选出梦寐以求的配合（鸡煲：我要学猛虎下山！）。这些数值设计虽看似微小，但能显著优化特定职业的前期体验，引导玩家向职业核心流派靠拢。 🔹 3.2 罕见遗物与Boss遗物则通过数值质变和机制突破催生全新策略，同时兼顾对单一策略的抑制​ 钢笔尖设定“每第十张攻击造成双倍伤害，将“低费复用输出的运转转化为攻击乘区，引导玩家调整高费攻击卡排序；蛇眼作为boss遗物，效果是每回合多抽2张，但所有卡牌的费用会从0到3费随机，解决了过牌问题，也让玩家多抓高费卡少带低费卡。 ​ 而针对飞身踢无限等可能导致策略退化的玩法，游戏通过数值限制进行平衡——飞身踢基础伤害仅5点（低于初始打击的6点），且需目标有易伤效果才能打出，提升了连携成本；同时Boss“时间吞噬者”设定玩家每打出12张牌后强制结束回合，从机制上限制低数值无限的强度（比如亮剑【0费打3抽1】妙计【0费防2抽1】相互抽的低数值无限），确保各流派的平衡。 📊 四、难度梯度数值：动态平衡的“学习型”挑战曲线​ 《杀戮尖塔》的进阶等级系统是其数值难度设计的核心，最高20级的进阶梯度并非简单提升敌人血量与伤害，而是通过“精准数值微调+规则约束增加”的方式，扰动核心数值循环，构建平滑且富有深度的学习型挑战曲线。其设计逻辑与玩家策略成长相匹配，低进阶等级调整普通敌人数值，引导玩家熟悉卡牌与遗物的基础连携；高进阶等级则通过规则约束强化数值压力，迫使玩家优化卡组构建策略。 🔹 4.1 进阶难度的数值设计​ 遵循蝴蝶效应原则，通过微小的数值调整引发策略连锁反应。例如进阶17后，二层普通敌人三鸟的坠机条件从受击3次变为受击4次，看似仅增加1次受击需求，却直接提升了对运转维度与攻击频率的要求（经典是观者的发泄，不敲三段伤害，敲了4段伤害，在低进阶一个发泄就可以把三鸟肘下来，在高进阶则刚需升级）——低费高频攻击卡组需多花费1回合才能击落，可能导致暴露在敌方多轮攻击下，增加生命值消耗。 ​ 而进阶20中，普通敌人壳爹＋蘑菇（开幕21和他的易伤药，已经畏惧了 …）第一回合伤害从18点提升至21点并附加脆弱Debuff），这一调整直接放大了防御维度的价值缺口，迫使玩家必须在前期就构建足够的防御体系，否则极易血量崩盘。这种差一点就通关的数值设计，既不会让玩家产生挫败感，又能推动玩家深化对数值体系的理解，优化策略决策以适配更严苛的数值循环。 🔹 4.2 高进阶等级的规则约束​ 进阶10后，玩家初始生命上限减少，这一数值调整直接提升了前期生命值资源的稀缺性，让血量管理成为核心决策点，同时放大了燃烧血、羽毛、梨子、华夫饼等回血与生命上限提升类遗物的价值（当然丢人枕头除外）；进阶15后“商店卡牌价格提升”，则扰动了经济数值循环，迫使玩家更谨慎地分配金币资源，优先选择移除低价值初始卡牌（指打击）而非盲目购买新卡。 这些设计形成了难度约束与策略适配的动态平衡，确保玩家在提升技巧的同时，始终能获得匹配的挑战体验。 🎯 结语：数值驱动的策略乐趣闭环​ 《杀戮尖塔》的数值设计之所以精妙，在于其构建了一个“价值基准-连携放大-循环闭环-难度适配”的完整体系：以能量价值曲线为基础基准，通过卡牌的“基础输入-乘区-导出”架构实现策略落地，借助遗物优化或打破基础规则以丰富流派，再通过难度调整扰动数值循环，推动玩家深化策略理解。 ​ 整个体系的核心是生命值等价原则，所有资源最终都可转化为生命值的留存能力，而每一处数值细节都服务于策略决策的核心乐趣——让玩家在随机生成的尖塔中，通过解读数值关系、组合连携模块、适配数值循环，不断探索全新的构筑可能性。这种以数值为骨架，以策略为血肉的设计思路，不仅避免了策略退化，保证了多流派的平衡共存，更成就了《杀戮尖塔》的经典地位，为后续卡牌Roguelike游戏的数值设计提供了可借鉴的核心框架。","categories":["游戏"]},{"title":"AI x game 深度体验报告","path":"/posts/2c281e1a/","content":"AI x game体验报告——是不可替代还是锦上添花？一、背景AI技术近年来发展迅猛，其迭代进程与应用成果已深深融入社会生活各领域，给大众带来了直观且深刻的感知。在此背景下，将AI技术与游戏产业相结合的创作理念逐渐成为热点，各类融合AI元素的新兴游戏产品如同雨后春笋般涌现。 不过，虽然很多游戏都使用AI，但他们的设计理念却不尽相同。有些是利用AI增添新的模块，像是在游戏里增加了接入大模型的可交互的NPC，增加UGC中的AI创作辅助；有些是基于AI构建新的玩法，比如构建一个全部由AI agent构成的社区，观察或参与他们的行为，在这个游戏里，AI是构建整个玩法的底层逻辑；或者更加简单通用一点，只是利用AI进行美术建模和文本的辅助创作……这些游戏到底怎么样，好不好玩，对AI的使用又究竟到了哪一步？我将在其中选择几种具有代表性的进行体验。 二、游玩体验1. AI-陪伴类：情绪价值的提供者现代社会中，快节奏的学习与工作节奏使得部分群体面临人际交往困境，或因社交适应障碍，或因缺乏有效倾诉对象，其情绪宣泄需求难以得到充分满足。在此背景下，在游戏场景中植入AI陪伴功能的设计理念应运而生，为解决此类群体的情绪需求提供了新的载体。 1.1 王者荣耀的灵宝系统和平精英AI战犬玩法2024年2月，王者荣耀上线了灵宝系统；25年11月，和平推出了AI战犬模式。直到今日，灵宝系统也一直在优化与迭代升级，就我个人的体验来说，我还是非常喜欢这个几个小家伙的。为什么这么说呢，我认为这种局内陪伴满足了以下几个优点： 1.1.1 游戏操作正反馈，提供情绪价值尤其在自己单排的时候，4个队友全部闭麦很常见。无论在你打出精彩操作拿到完美击杀还是在大逆风时刻出其不意抢下大龙，队友不一定会夸你，都在闭麦干自己的事。这无可厚非，但没有人为你欢呼 难免会失落。但有灵宝就不一样，他夸奖人来比谁都快比如我玩兰陵王喜欢开局反（tou）对面红buff，每每打完红之后听灵宝来一句：“拿下对面红buff，对面打野恐怕要气疯了吧” 欸嘿，心情就会非常舒坦。 1.1.2 局内提供帮助，利好休闲玩家王者荣耀与和平精英作为国民级手游，拥有庞大的玩家群体，这进一步导致了休闲玩家的比例占绝大多数。相比于硬核玩家，他们的游戏技术或意识显得会比较低。那如何更好的照顾到这类玩家的游戏体验呢？局内陪伴的另一个作用便显现出来了。这里那和平的战犬举例，给出对应的语音指令，战犬便会帮忙收集物资，牵制敌人，或者救助队友，很好的避免了轻度玩家进入游戏后的手忙脚乱。并且他的功能性设计，也不会对游戏平衡性造成很大的破坏。对我这个新手来说，算是帮大忙了 1.1.3 活跃游戏气氛，调节玩家情绪在现代社会中，比完整的游戏时间更难得的是有个和你一起打游戏的朋友，不一定两个人都有时间。所以很多时候我都是单排自己玩，难免会感到寂寞，此时局内陪伴的作用就来了，时不时来两句插科打诨，也可博君一笑。 1.2 星之低语EVE（后者目前只有二测演示视频）1.2.1 whispered from the star该产品是由蔡浩宇主导开发的AI对话类游戏，核心玩法围绕玩家与游戏内漂泊太空的女性角色展开全程对话交互，通过AI的引导与玩家的决策推进剧情。我体验了两个小时，怎么说呢，个人觉得这更像是一款实验品，验证人机交互逻辑和语音识别的先行者，而不是一个完整的游戏。从目前的技术力来看，星之低语的交互建模语音生成乃至情感叙事无疑都走在同类型赛道的前列。但是问题在于，游戏性的缺失导致这不像一个游戏，更像是一次英语口语练习（笑）。其体验的瓶颈也显而易见：后期的交互易陷入重复，缺乏真正的情感弧线与剧情张力。当然啦，它的技术力很强也是无可反驳的。我目前更倾向于认为这是一款公开的试验田，为以后的模型收集数据和反馈，期待他们的下一款游戏。 1.2.2 EVE相比于上面一款，这个游戏的理念更加贴近于传统AI陪伴的概念。该产品以构建3D虚拟伴侣为核心，规划实现对话交互、语音通话、生活服务提醒（如点奶茶、购药等）等功能。由于该产品计划于今年三月公测，且优先面向女性向用户群体，我的体验仅基于测试玩家发布的演示视频进行分析。 从行业发展视角来看，《EVE》的产品形态本身并非我们的关注核心，其背后的设计思路更具行业参考价值。该产品精准命中了AI陪伴领域的核心潜在需求——为用户提供符合个性化想象的、以用户为中心的虚拟伴侣。无论《EVE》正式上线后的市场表现如何，其开创的产品方向必然会引发行业内其他游戏工作室的跟进。社会调查数据显示，大龄单身青年群体规模持续扩大，此类群体的情感需求并未因单身状态而消失，对情感替代载体的需求日益凸显。而具备高真实度交互能力的AI虚拟伴侣，恰好能够满足这一需求。未来，该赛道有望涌现更多高技术力产品，但如何实现产品差异化竞争，将成为我们需要深入思考的核心问题。 2. AI NPC：最多的可能性，但被技术力约束曾经，NPC所说的话总是由文案策划构思好，虽然这对于游戏的推进来说是最快捷的方式，但看多了不免感到乏味。随着LLM的兴起，大家不约而同的想到那个可能性————让大模型控制NPC的交流行为甚至剧情。 2.1 逆水寒燕云十六声在我印象里，逆水寒最早大规模宣传自己的AI npc，当时我也是因为这个宣传下载体验了一番汴京的风土人情。当时也涌现了很多与AI对话的“焚诀”，和ai npc斗智斗勇也成了每天上线不得不品的一环。乐趣确实有，也能通过文字交流得到npc的物品，在当时给我感觉还是比较新奇有趣的。 比较可惜的是，这些玩法没有更进一步，你和npc的对话对游戏主体的影响微乎其微，也基本在主线剧情中得不到体现。在这两款游戏里，AI不是一个不可代替的玩法，只是一个外接的模块，可以说删了这个功能换成传统的npc对话形式也不会对游戏产生多大影响。只能说这是受限于目前的技术力，还无法做到AI影响的多剧情分支走向（也可能是不方便管理？） 2.2 AI bot，觉悟人机这就比较的传统并且成熟了，在游戏对局内补充人机有利于减少玩家匹配时间，控制对局强度，也有利于新手玩家熟悉游戏，由于在大部分游戏里已经有了较为成熟的应用，本文不再赘述 3. AI作为游戏基石：未来将至？以上是在传统的游戏中增添AI的元素，那么我们有没有可能以AI为底层逻辑创建一个游戏呢？很多影视文学作品都给出过设想，例如头号玩家，失控玩家以及前几年较火的元宇宙概念，他们都给我们提供了一种可能性。我也很高兴的看到有很多游戏在向这方面发展，例如斯坦福小镇，avilizition以及喵吉托的几款游戏demo（例如喵呜岛）。 3.1 avilizition的体验这个项目更准确地应该称为一个模拟平台。它的核心理念非常吸引人（也是我所展望追求的）：每个智能体都有独立的记忆、人际关系和日程，所有的决策都具有蝴蝶效应，npc并能根据与环境和彼此的互动来动态决策。 我的体验过程充满了新奇与困惑。一开始，我就像进入了一个上帝视角的观察者，看着这些像素小人在小镇里活动，他们会上班、购物、聊天、举办活动。最让我感到惊讶的是，他们之间的互动确实能产生一些预设脚本之外的故事。比如，我看到两个角色因为在咖啡馆的一次聊天而决定晚上一起去酒吧，或者因为之前的某次争执而在下一次见面时显得冷淡。这种基于记忆和关系链产生的行为变化，是它最核心的亮点，让你感觉这个小镇是活的，有自己的时间流。 但是，这种新鲜感褪去得也很快。首先是交互的乏力感。作为玩家，我能与这些AI角色对话，但对话内容往往流于表面，很难进行有深度的、能实质性改变他们目标或关系的交流。更多的时候，我感觉我是在试图引导对话，而不是在进行有意义的角色扮演。avilizition给我的感觉是，它是一座通往未来游戏理念的桥梁设计图，但我们脚下的建筑材料还不足以把它扎实地建造起来。它足以证明了AI作为游戏世界底层逻辑的可行性，但要把它变成一款好玩的游戏，还有很长的路要走。 三、总结经过这些体验，我认为目前AI在游戏中的应用，整体上仍然处于“锦上添花”的阶段。无论是提供陪伴的灵宝、战犬，还是能自由对话的NPC，它们确实带来了新鲜感和更好的体验，像是给原有的蛋糕裱上了更精美的花。但如果我们把花拿走，蛋糕本身并没有改变。它们更多是体验的优化与内容的补充。 然而，从《星之低语》、《EVE》这类纯粹的AI交互应用，尤其是像avilizition这样以AI为世界基石的尝试中，我们能看到一种截然不同的可能性。它们不再满足于做蛋糕上的点缀，而是试图用AI这种新材料，从头烤制一块全新的、我们从未尝过的点心。虽然目前这些可能还有些生涩，形态也不稳定，但方向已经指明。 所以，我的看法是：未来，AI完全有可能成为构建游戏里不可替代的基石。这条路很长，但起点已经在我们脚下。","categories":["游戏"]},{"title":"刚体运动的python解法","path":"/posts/48befe4f/","content":"自由转动杆与铰链固定杆倾倒时间对比研究一、问题背景​\t某p大友人中午问了我一个问题 ​ 曾经自诩“高中物理领域神棍”的我也是体会了一把修为尽失的感觉…….可恶，还是，做不到吗。。。 才怪！现在我有了新的武器，解析不了我就暴力数值解嘻嘻 二、物理模型2.1 自由杆（光滑地面） 约束条件：底端无水平约束，质心水平方向动量守恒 运动特点：质心同时有平动和转动 能量方程：$E \\frac{1}{2}mv_c^2 + \\frac{1}{2}I_c\\omega^2 + mg\\frac{L}{2}\\cos\\theta$ 2.2 铰链杆（固定底端） 约束条件：底端位置固定，绕该点纯转动 运动特点：质心作圆弧运动 能量方程：$E \\frac{1}{2}I_o\\omega^2 + mg\\frac{L}{2}\\cos\\theta$ 三、动力学方程推导3.1 自由杆的倾倒时间# 自由杆的角速度公式推导ω² = (4g/L) * (cosθ₀ - cosθ) 3.2 铰链杆的倾倒时间# 铰链杆的角速度公式推导ω² = (3g/L) * (cosθ₀ - cosθ) 四、数值计算方法4.1 时间积分公式倾倒时间通过对角速度倒数积分得到：$$ t \\int_{\\theta_0}^{\\pi2} \\frac{1}{\\omega(\\theta)} d\\theta $$ 4.2 数值实现代码自由杆计算代码import numpy as npg = 9.81L = float(input(请输入杆长 L (米): ))theta0_deg = float(input(请输入初始倾斜角度 (0–90°, 相对竖直): ))theta0 = np.deg2rad(theta0_deg)theta_end = np.pi / 2def integrand(theta): omega = np.sqrt((4 * g / L) * (np.cos(theta0) - np.cos(theta))) return 1.0 / omegathetas = np.linspace(theta0 + 1e-6, theta_end, 200000)t = np.trapezoid(integrand(thetas), thetas) 铰链杆计算代码import numpy as npg = 9.81L = float(input(请输入杆长 L (米): ))theta0_deg = float(input(请输入初始倾斜角度 (0–90°, 相对竖直): ))theta0 = np.deg2rad(theta0_deg)theta_end = np.pi / 2def integrand(theta): omega = np.sqrt((3 * g / L) * (np.cos(theta0) - np.cos(theta))) return 1.0 / omegathetas = np.linspace(theta0 + 1e-6, theta_end, 200000)t = np.trapezoid(integrand(thetas), thetas) 五、模拟结果分析5.1 不同初始角度对比（默认杆长1m） 初始角度 自由杆时间(s) 铰链杆时间(s) 时间比(自由铰链) 1° 1.1837 1.3668 想算的自己算 10° 0.6648 0.7676 30° 0.4163 0.4807 (— __ —) 45° 0.3201 0.3697 60° 0.2431 0.2807 80° 0.1338 0.1545 5.2 不同杆长对比（默认倾斜30°） 杆长(m) 自由杆时间(s) 铰链杆时间(s) 时间比 0.5 0.2944 0.3399 1.0 0.4163 0.4807 2.0 0.5888 0.6799 5.0 0.9309 1.0750 六、结论自由杆下落更快！ 但原因是什么呢 有没有能给我推导过程的QAQ 在线等~ 附录：论AI目前为什么不能取代人类请看vcr 1.《有固定点，小角度？那就是简谐运动！》————来自亲爱的deepseek 2.《什么是物理，我听不懂，倾斜的杆子就是不会掉下来，牛顿说啥我不管》 ————GPT5 plus大人！\\0 建议加入ai笑话大全~","categories":["编程"]},{"title":"python网络编程--TCP篇","path":"/posts/8d64c14e/","content":"TCP网络编程初步在学习并发编程之前，先熟悉一个客户端与一个服务端 TCP协议核心机制TCP（传输控制协议）通过三次握手建立连接，通过四次挥手终止连接，确保数据传输的可靠性。 服务端实现代码import socketserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 服务端的socket IPV4 TCP# 这个只负责接受客户端的连接请求server_socket.bind((, 8000))# 允许最大的等待个数server_socket.listen(128)# 接受客户端的连接socket2, client_addr = server_socket.accept()while True: msg = socket2.recv(1024).decode(utf8) if msg == quit: break print(f来自客户端IP：client_addr[0],端口号client_addr[1]:msg) # 给客户端发送聊天信息 send_msg = input(server) socket2.send(send_msg.encode(utf8))socket2.close()server_socket.close() 客户端实现代码import socketclient_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)server_addr = (192.168.100.199, 8000)client_socket.connect(server_addr)while True: send_msg = input(client) if send_msg == quit: client_socket.send(send_msg.encode(utf8)) break client_socket.send(send_msg.encode(utf8)) msg = client_socket.recv(1024).decode(utf8) print(f来自服务端IP：server_addr[0],端口号server_addr[1]:msg)client_socket.close() TCP三次握手详解三次握手是TCP建立连接的过程，确保双方都有发送和接收能力。 第一次握手：SYN客户端 → SYN=1, seq=x → 服务端 客户端发送SYN包，seq为随机数x，进入SYN_SENT状态。 第二次握手：SYN+ACK客户端 ← SYN=1, ACK=1, seq=y, ack=x+1 ← 服务端 服务端收到SYN，发送SYN+ACK包： seq为随机数y ack为x+1（确认收到客户端的x）服务端进入SYN_RCVD状态。 第三次握手：ACK客户端 → ACK=1, seq=x+1, ack=y+1 → 服务端 客户端发送ACK包： seq为x+1 ack为y+1（确认收到服务端的y）双方进入ESTABLISHED状态，连接建立。 代码中的三次握手# 客户端发起连接（第一次握手）client_socket.connect(server_addr)# 服务端接受连接（完成三次握手）socket2, client_addr = server_socket.accept() TCP四次挥手详解四次挥手是TCP断开连接的过程，确保双方数据都传输完毕。 第一次挥手：FIN客户端 → FIN=1, seq=u → 服务端 客户端发送FIN包，seq为u，进入FIN_WAIT_1状态。 第二次挥手：ACK客户端 ← ACK=1, seq=v, ack=u+1 ← 服务端 服务端发送ACK包： ack为u+1（确认收到客户端的FIN）服务端进入CLOSE_WAIT状态，客户端进入FIN_WAIT_2状态。 第三次挥手：FIN客户端 ← FIN=1, ACK=1, seq=w, ack=u+1 ← 服务端 服务端发送FIN+ACK包，进入LAST_ACK状态。 第四次挥手：ACK客户端 → ACK=1, seq=u+1, ack=w+1 → 服务端 客户端发送ACK包，进入TIME_WAIT状态（等待2MSL），服务端关闭连接。 代码中的四次挥手# 客户端发送quit（发起第一次挥手）client_socket.send(quit.encode(utf8))# 服务端收到quit（第二次挥手）msg = socket2.recv(1024).decode(utf8)if msg == quit: break# 关闭套接字（完成挥手过程）socket2.close()server_socket.close()client_socket.close() TCP状态转换建立连接状态 CLOSED：初始状态 SYN_SENT：客户端发送SYN后 LISTEN：服务端调用listen()后 SYN_RCVD：服务端收到SYN后 ESTABLISHED：连接建立完成 断开连接状态 FIN_WAIT_1：客户端第一次挥手 CLOSE_WAIT：服务端第一次收到FIN FIN_WAIT_2：客户端收到ACK LAST_ACK：服务端发送FIN TIME_WAIT：客户端最后等待 CLOSED：连接完全关闭 为什么需要三次握手？防止旧的重复连接初始化 避免网络延迟导致的旧连接干扰新连接 确保双方都知道对方准备好了 同步序列号 交换初始序列号（ISN） 确保数据按序传输 确认双方能力 确认双方都有发送和接收能力 协商窗口大小等参数 为什么需要四次挥手？半关闭状态 TCP是全双工的，可以单向关闭 服务端可能还有数据要发送 确保数据完整性 等待所有数据都传输完毕 确保没有数据丢失 可靠终止 双方都知道连接要关闭了 防止数据包在网络中”迷路” TIME_WAIT状态的作用等待2MSL的原因 确保最后一个ACK到达：如果服务端没收到ACK会重发FIN 让旧连接的数据包消失：防止影响新连接 MSL（Maximum Segment Lifetime）：报文最大生存时间 实际编程中的影响连接建立失败try: client_socket.connect(server_addr)except ConnectionRefusedError: print(连接被拒绝：服务端未启动或端口错误) 端口复用问题# 避免Address already in use错误server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) 关闭# 半关闭：关闭发送，仍可接收client_socket.shutdown(socket.SHUT_WR)# 等待对方关闭while True: data = client_socket.recv(1024) if not data: break 总结三次握手建立可靠连接，四次挥手优雅终止连接，这是TCP协议的核心机制。理解这些过程对于调试网络问题和编写稳定的网络程序非常重要。 在代码中，connect()触发三次握手，close()触发四次挥手，这些细节都由操作系统自动处理。","categories":["编程"]},{"title":"UDP的简易实现","path":"/posts/35e240ca/","content":"PYTHON网络编程——UDP篇一、UDP简介什么是UDP？UDP（用户数据报协议）是一种无连接的传输层协议，具有以下特点： 无需建立连接 不可靠传输（可能丢包） 传输速度快 适合实时性要求高的场景 基本流程对比TCP编程流程：创建socket → 绑定 → 监听 → 接受连接 → 收发数据 → 关闭UDP编程流程：创建socket → 绑定（服务端）→ 直接收发数据 → 关闭 二、服务端代码详解import socket# 创建了一个UDP的socket对象server_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# socket.AF_INET: 使用IPv4地址族# socket.SOCK_DGRAM: 使用UDP协议（数据报套接字）# 前一个是我的ipv4地址，如果用127.0.0.1则是在本地# 如果是空字符，服务端绑定到所有的ip地址server_socket.bind((192.168.100.199, 6666))# bind()方法将套接字绑定到指定地址和端口# 192.168.100.199: 绑定到特定局域网IP# 其他选择：# 127.0.0.1: 本地回环，只能本机访问# 0.0.0.0: 绑定所有网络接口# : 同0.0.0.0# 6666: 端口号，范围0-65535（0-1023为系统保留）while True: # msg是收到的数据，addr是源地址和端口号 msg, addr = server_socket.recvfrom(1024) # recvfrom()是阻塞方法，会一直等待直到收到数据 # 1024: 缓冲区大小，单位字节 # msg: 接收到的字节数据 # addr: 元组 (客户端IP, 客户端端口) if msg.decode(utf8) == quit: break # 如果客户端发送quit，则退出循环 print(f来自IP：addr[0],端口号：addr[1]的消息：msg) # 显示消息来源和内容 send_msg = input(服务端) # 等待用户输入回复内容 # 不能发送字符串，应该是字节数据 server_socket.sendto(send_msg.encode(utf8), addr) # sendto()发送数据到指定地址 # encode(utf8): 将字符串转为字节数据 # addr: 目标地址（这里发回给原客户端）# closeserver_socket.close()# 关闭套接字，释放资源 三、客户端代码详解import socket# 创建了一个UDP的socket对象client_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# 客户端socket不用bind# 客户端通常由系统自动分配端口号# 如果需要绑定特定端口，也可以使用bind()while True: # send msg send_msg = input(客户端) if send_msg == quit: break # 输入quit退出 client_socket.sendto(send_msg.encode(utf8), (192.168.100.199, 6666)) # 发送数据到服务器 # (192.168.100.199, 6666): 服务器地址和端口 # 必须与服务器绑定的地址一致 # receive msg msg, addr = client_socket.recvfrom(1024) # 等待服务器回复 # 注意：这里会阻塞，直到收到服务器响应 print(f来自服务端IP：addr[0],端口号：addr[1]的消息：msg) # 显示服务器回复client_socket.close()# 关闭客户端套接字 四、关键知识点1. UDP套接字创建socket.socket(socket.AF_INET, socket.SOCK_DGRAM) AF_INET: IPv4地址族 SOCK_DGRAM: UDP数据报类型 2. 地址绑定# 服务端必须绑定，客户端可选server_socket.bind((IP地址, 端口号)) 3. 数据收发# 接收数据（返回数据和来源地址）data, addr = socket.recvfrom(缓冲区大小)# 发送数据到指定地址socket.sendto(字节数据, (目标IP, 目标端口)) 4. 编码转换# 发送：字符串 → 字节send_msg.encode(utf8)# 接收：字节 → 字符串msg.decode(utf8) 五、注意事项 UDP是无连接的 每次发送都要指定目标地址 不保证数据顺序和可靠性 地址和端口 服务端需要固定端口 客户端端口通常由系统分配 数据大小 UDP数据包不宜过大（通常1500字节） 避免IP分片，提高传输效率 阻塞问题 recvfrom()是阻塞调用 程序会等待直到收到数据 多客户端处理 UDP服务端可以同时处理多个客户端 通过addr区分不同客户端 六、常见问题Q1: 为什么客户端不需要bind？A: 客户端第一次调用sendto()时，系统会自动分配一个可用端口。 Q2: 如果服务器没启动，客户端会怎样？A: 客户端发送的数据会丢失，recvfrom()会一直等待（阻塞）。 Q3: 如何测试本机通信？A: 服务端绑定127.0.0.1，客户端连接127.0.0.1。 Q4: 如何让其他电脑连接？A: 服务端绑定0.0.0.0或局域网IP 关闭防火墙或开放对应端口 客户端使用服务器实际IP地址 七、完整的UDP还需要 异常处理：添加try-except处理网络错误 超时设置：使用settimeout()避免永久阻塞 多线程：同时处理多个客户端请求 数据验证：添加简单的协议头验证数据完整性 八、总结​ UDP编程的核心模式： 服务端：创建→绑定→循环收发→关闭 客户端：创建→循环收发→关闭 这种简单的请求-响应模式是UDP编程的基础，理解了这种模式后，可以在此基础上构建更复杂的UDP应用。 注：实际使用时，请确保服务端和客户端的IP地址和端口号配置正确，防火墙已开放相应端口。","categories":["编程"]},{"title":"如何用hexo搭建个人博客","path":"/posts/cd1cb590/","content":"Hexo 博客搭建与配置指南📖 简介Hexo 是一个快速、简洁且高效的静态博客框架。它使用 Markdown解析文章，在几秒内即可生成静态网页。本指南将帮助您快速搭建并配置一个 Hexo 博客。 🚀 快速开始环境准备 安装 Node.js 版本要求：Node.js 14.0 或以上版本 下载地址：Node.js 官网 安装 Git 下载地址：Git 官网 安装 Hexo打开终端（命令提示符）并执行： npm install -g hexo-cli 初始化博客hexo init my-blogcd my-blognpm install 本地预览hexo clean# 生成：hexo g# 预览： （浏览器访问 `http://localhost:4000` 查看效果）hexo s# 部署：hexo d ⚙️ 基本配置配置文件博客的主要配置文件为 _config.yml，位于博客根目录下。 常用配置项# 网站信息title: 我的博客 # 网站标题subtitle: # 网站副标题description: # 网站描述keywords: # 网站关键词author: 作者名 # 作者名称language: zh-CN # 语言timezone: # 时区# URL 设置url: http://yoursite.com # 网站地址root: / # 网站根目录permalink: :year/:month/:day/:title/ # 文章永久链接格式# 目录设置source_dir: source # 资源文件夹public_dir: public # 静态文件生成目录# 写作设置new_post_name: :title.md # 新文章文件名格式default_layout: post # 默认布局 🎨 主题安装与配置安装主题以我所使用的stellar 主题为例： cd my-bloggit clone https://github.com/stellar-theme/hexo-theme-syellar themes/stellar 启用主题修改 _config.yml： theme: stellar 主题配置主题有自己的配置文件 themes/stellar/_config.yml，可进行个性化设置。 📝 写作与发布创建新文章hexo new 文章标题 文章文件位于 source/_posts/文章标题.md 文章 Front-matter---title: 文章标题date: 2024-05-20 10:00:00tags: [标签1, 标签2]categories: 分类名--- 常用命令 hexo new [layout] title - 新建文章 hexo clean - 清除缓存 hexo generate 或 hexo g - 生成静态文件 hexo server 或 hexo s - 启动本地服务器 hexo deploy 或 hexo d - 部署到远程仓库 🌐 部署到 GitHub Pages准备工作 创建 GitHub 仓库，命名为 用户名.github.io 安装部署插件： npm install hexo-deployer-git --save 配置部署修改 _config.yml： deploy: type: git repo: https://github.com/用户名/用户名.github.io.git branch: main 部署命令hexo cleanhexo generatehexo deploy 🔧 常用插件# RSS 订阅npm install hexo-generator-feed --save# 站点地图npm install hexo-generator-sitemap --save# 文章搜索npm install hexo-generator-searchdb --save# 文章字数统计npm install hexo-wordcount --save 💡 实用技巧自定义页面hexo new page about 草稿功能hexo new draft 草稿标题hexo publish 草稿标题 图片引用 将图片放入 source/images/ 目录 在文章中引用： ![图片描述](/images/图片文件名.jpg) 🛠️ 故障排除常见问题 部署失败 检查 Git 配置 确认仓库地址正确 检查网络连接 页面无法加载 运行 hexo clean 重新生成和启动服务器 主题不生效 确认主题文件夹名称正确 检查 _config.yml 中 theme 配置 📚 学习资源 Hexo 官方文档 Hexo GitHub Next 主题文档 提示：配置时建议备份原配置文件，每次只修改少量配置并测试效果。"},{"title":"关于我 | 黄逸飞的个人小站","path":"/about/index.html","content":"👋 关于我​\t米娜桑，扣尼吉娃（大佐音） 在下黄逸飞是也。 ​\t目前就读于中国科学技术大学本科人工智能专业。很荣幸能和你在这里相遇。 ​\t这里是我个人的小站，记录自己的学习和生活 💻 我想分享的事 编程： 主要使用python，应用于网络编程，大模型应用开发方向； 兼顾C和java（学校教的，额，用于数据结构与算法，OS，等基础课程，可能不太会在博客分享，主要我感觉有些无聊。。。也许leetcode上遇到有意思的算法题会写上去，嗯，就这样）； 会一点点js（当然是指能看懂LLM给我写了什么） 阅读： 爱看小说，爱看散文，爱看杂文。杂食党，什么都看~ 也许会把最近看的书写在博客上？不过看的轻小说就不会和你们分享了，不行不行绝对不行！（**不是不可能？） 游戏： AI与游戏的结合（AI x GAME）是我一直感兴趣的方向，也思考过很多可能性 梦想是当游戏策划（不想敲代码版） 📌 小站的意义这个博客是我记录生活、分享思考的自留地，分享我的思考，记录我的旅程。 如果你也对这些话题感兴趣，欢迎常来逛逛、留言交流～ 愿我们都能在热爱的领域里闪闪发光，也能在平凡的日子里收获属于自己的小确幸！ ✨ 感谢相遇，期待成为朋友！"}]